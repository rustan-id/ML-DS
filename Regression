Course Overview
Course Overview
(Music) Hi. My name is Janani Ravi, and welcome to this course on Building Regression Models with scikit-learn. A little about myself. I have a master's degree in electrical engineering from Stanford, and have worked at companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real-time collaborative editing in Google Docs, and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high-quality video content. Regression is one of the most widely used modeling techniques, and it's much beloved by everyone ranging from business professionals to data scientists. Using scikit-learn, you can easily implement virtually every important type of regression with ease. In this course, you will gain the ability to enumerate in different types of regression algorithms and correctly implement them in scikit-learn. First, you will learn what regression seeks to achieve and how the ubiquitous ordinary least squares algorithm works under the hood. Next, you will discover how to implement other techniques that mitigate overfitting, such as lasso, ridge, and elastic net regression. You will then understand other more advanced forms of regression, including those using support vector machines, decision trees, and stochastic gradient descent. Finally, you will round out the course by understanding the hyperparameters that these various regression models possess and how these can be optimized. When you're finished with this course, you will have the skills and knowledge to select the correct regression algorithm based on the problem you're trying to solve and also implement it correctly using scikit-learn.

Understanding Linear Regression as a Machine Learning Problem
Version Check
Module Overview
Hi, and welcome to this course on Building Regression Models with scikit-learn. In this module, we'll get started with understanding linear regression as a machine learning problem. Linear regression is a technique that has existed for ages, but it can be considered to be the first thing that you'll encounter when you start your study of machine learning. We'll understand what exactly linear regression means, and we'll see how it can be used to express relationships in your data. We'll see how we can find the best fit straight line to represent our data by minimizing the mean square error or the least square error as the loss function. We'll see how we can mathematically express the best fit line as the one which has the least mean square error. Once we've set up our linear regression problem, we'll then talk about how we can interpret the results of a regression analysis, we'll discuss the risks involved in simple regression and multiple regression and talk about techniques to mitigate these, and we'll also discuss in some detail R squared for evaluating our regression models, and how the adjusted R squared is preferred for multiple regression.

Prerequisites and Course Outline
Before we dive into the actual course contents, let's take a look at some of the prereqs that you need to have to make the most of your learning. This course assumes that you're familiar with basic Python programming because that's what you're going to use to build our models. You don't really require prior ML exposure, though it would be helpful for you to have a basic understanding of the machine learning workflow. The only math that you need for this course is high school math, nothing else, and you should be good with that. If you're completely new to the idea of machine learning, here are some other courses on Pluralsight that you might want to watch first so that you're not lost anywhere during this course. Building Your First scikit-learn Solution is a prereq for this course. We start this course off by first understanding the regression problem and how we can use linear regression to express relationships in our data. We'll then move on to building simple regression models using the scikit-learn library. We'll then talk about the risks involved with over fitting on your training data and see how we can build regularized regression models to mitigate over fitting. We'll then move on to considering many different advanced integration techniques using a variety of ML algorithms, support vector regressors, nearest neighbor regressors, least angle regression, and so on. And finally, we'll round this course off by discussing hyperparameter tuning that you can use to design and fine tune your regression models.

Connecting the Dots with Linear Regression
As a data scientist or a data analyst, your work is mostly about connecting the dots. Linear regression is one of the tools that you can use to do so. A powerful person once said, "My mind is made up, don't confuse me with the facts. " Well, I don't really agree with him, and I'm glad that there are many people out here in the real world today who don't. As someone who works with data, you will often be called upon to present your thoughtful, fact-based point of view, fact-based because it has to be based on real data, which has been painstakingly collected. Thoughtful because it has to be balanced, weighing the pros and cons of the recommendation that you're making. And finally, the point of view is important because you will be asked upon to make a prediction, recommendation or a call to action. In order to present this thoughtful fact- based point of view, you have two sets of statistical tools at your disposal. Descriptive statistics, which you'll use to identify important elements in a dataset, summarize and quantify the dataset such as find the mean, mode, median, the range, the inter quartile range, variants and so on. And the second set of tools, which goes hand in hand with descriptive statistics, is inferential statistics. These are the tools that you would apply to explain the important elements that you identified via relationships with other elements. Hypothesis testing of proposed explanations, data modeling, these are all inferential statistics. These two sets of tools correspond exactly with the two hats of a data professional, finding the dots, identifying important elements in a dataset, and connecting the dots, explaining those elements via relationships with other elements. This is what helps us draw insights, this is what helps us make decisions. It's possible that in the real world, you're often dealing with data in one dimension. Unidimensional data points can be represented using a single line, such as a number line and the points that you see plotted here on screen. Such data is typically analyzed using statistics such as mean, median, and standard deviation. These are called descriptive statistics and they are referred to as measures of central tendency and measures of dispersion. The mean the median define the central point of your data in some way and the standard deviation gives you an idea of how those numbers jump around. But often it's far more useful and insightful to view data in relation to some other data that is related to the original data. This is bidimensional data that can be represented on a coordinate plane. We have government bond yields on the x axis and oil prices. How do government bond yields affect oil prices? That's a good question. Now we can draw any number of curves to fit such data to understand these relationships. All of these curves would represent how government bond yields affect oil prices, how do they move together. If the line that you use to express the relationship between these two variables is a straight line, that is a linear relationship. But with just two variables in our data, it's possible to have all kinds of relationships need to fit that data. We could meet this curve pass through each point, each data point that you have in your dataset. This of course is an arbitrarily complex curve, and such curves that track the original data points very closely are often called overfitted curves. In a sense, we could have any number of curves fit the data in aggregate. Given all of these choices, what you're really looking for is a curve with a good fit, and a curve can be considered to have a good fit if the distances of the points from the curves that you've drawn are small. The points are original data, the curve represents the relationship, and it's good if these distances are as small as possible. Let's say you have a really complex curve representing this relationship, and later on you have a new data point that comes in. If your curve is arbitrarily complex, that is overfitted, this has a major drawback if you're trying to use this curve for predictive analytics. Imagine that this curve is a model of your underlying data and overfitting by finding a very complicated curve often only hurts the predictive accuracy of your model. And you might find that in the real world often just a straight line works just fine to express your model or your relationship. And finding the best such straight line that can be used to express the relationships in your data is called linear regression. And this straight line is your linear model, or the model that fits your data. If you remember your high school math, you must have studied the equation of a straight line. The linear regression relationship can be expressed using the equation of a straight line, y is equal to A + Bx. Here, A is the intercept of this line and B is the slope. The objective of linear regression is to find these values, A and B, to express the linear relationship that best fits your data. Now regression not only gives us the equation of the line, but it also signals how reliable the line is, how well it fits the underlying data. Let's say your original data points as captured as you see here on screen, and this is the line that you've drawn to express the relationship that exists in this data. It's pretty obvious visually that the line that we've drawn has a high quality of fit. Such a line is said to have a high R squared. Let's say these were your original data points, and this is the line that you've drawn, such a line has a low quality of fit, it's said to have a low R squared. So this is important, we'll come to it over and over again in our demos. R squared is a measure of how well the linear regression fits the underlying data. How much of the variance in the underlying data points is captured by the linear relationship? R squared is a number between 0 and 1, which is often expressed as a percentage. Once you have a regression line to express the relationship that exists in your data, you can use it for prediction. You will have an out of sample value of x, you'll find the corresponding value of y using your line, and this is what your predicted y value is. When you perform regression using standard libraries, you'll find that often your regression analysis is also accompanied by prediction intervals similar to confidence intervals around the point estimate. All our discussion of linear regression has been with data in two dimensions, but regression can be extended to n-dimensional data very, very easily. So whether you have two, three, four or five, or any number of x variables that you want to use to predict the value of a y variable, regression is a useful tool in your toolkit.

Minimizing Least Square Error
Now that we understand linear regression, let's understand how we can set up a regression problem. Let's say you want to explore this relationship where you know that x causes y. X here is the cause or the independent variable, y is the effect or the dependent variable. X in this case is often referred to as the explanatory variable. Now, if you perform linear regression, this involves finding the best fit line between your cause and effect variable. This is the line that best expresses this relationship. So what is this best fit line? Assume that this is Line 1 with the formula, y is equal to A1 + B1x, and you have another line here, Line 2: y is equal to A2 + B2x. Back to a little bit of high school math. You can see that the first line here has an intercept of A1; A1 is the intercept and B1 is thought of as the slope of the line. What does B1 mean? If x increases by 1 unit, y decreases by -B1. That is what the slope of this line represents. Y - B1, as you can see, this is a downward sloping line, and that is why B1 is negative. The second line that we've drawn here, Line 2, has an intercept of A2, this is the y intercept, and the slope of B2 basically means that if x increases by 1 unit, y decreases by -B2 units. This is a downward sloping line once again, and that is B2 to is negative. Now that you've understood both of these lines, let's talk about finding the best fit line, and this involves minimizing something called the least square error of your line. So how does one minimize this least square error? For each of these lines, Line 1 and Line 2, drop vertical lines from the line to all of the data points that represent your dataset. So here are the vertical lines drawn from Line 1 and here are the vertical lines drawn from Line 2. Now you intuitively know that Line 1 is a better fit to express the relationship that exists in your data as compared with Line 2. Now let's express this mathematically. The best fit line is the one where the sum of the squares of the lengths of these dotted lines is a minimum. Consider these dotted lines, these are the lengths, these are the errors. These are the errors between the fitted line and the actual data points. The best fit line is the one where the sum of the squares of the lengths of the errors is minimum. And what we just expressed in words is the least square error, the best fit line is the one by the sum of the squares of the lengths of the errors is minimized, and finding this line is the objective of any regression problem. Once we have this best fit line, you can imagine that every data point in our original dataset can be projected onto this line. This new projected by value is called the fitted value for that corresponding x. That brings us to one more term that you need to be aware of. Residuals of the regression are the difference between actual and fitted values of the dependent variable, which in this case is y. When you use linear regression to find the best fit line, we need to make some assumptions about this regression error. There is a fine distinction between errors and residuals, but we can ignore it in order to get a high-level understanding of regression. Here's a quick overview of the implicit assumptions that we make when we fit a regression line on our data. Ideal residuals should have zero mean, a common variance, be independent of each other, be independent of x, and be normally distributed. The exact discussion of each of these are out of the scope of this particular course, but you should know that these are underlying assumptions.

Installing and Setting up scikit-learn
In this demo, we'll make sure that we have Python set up on our local machine, install the scikit-learn libraries if we need to, and explore our dataset. Before we get started building regression models in scikit-learn, let's ensure that we have the scikit-learn library installed on our machine and the right version of Python to work with on our demos. Make sure that you already have Python installed on your machine. The Python version that we'll be using in this course is 3.7 .1. Now, scikit-learn needs Python versions greater or equal to 2.7 or greater or equal to 3.4. The latest version of scikit-learn available at the time of this recording, that is version 0.20 .2, supports both of these Python versions. Going forward for newer scikit versions, version 0.2 and onwards, only Python versions greater than 3.5 will be supported. We'll write all of our code using Jupyter Notebooks, the browser-based interactive shell available for Python. The Jupyter version that I'm currently using is 4.4 .0. We'll use the pip Python package manager in order to install any libraries that we need. The pip version that I currently have on my machine is 18.1. If you do not have the scikit-learn libraries installed on your machine, you can get it using a simple pip install. Pip install -U scikit-learn will install scikit-learn and all of the other dependencies that it needs. The scikit-learn version that we're working with is 0.20 .2. Scikit-learn under the hood uses the NumPy library, as well as SciPy. These will automatically be installed along with scikit-learn. If you want to get them separately, you can simply do that using a pip install for NumPy, as well as for SciPy. With all of these package installs out of the way, let's bring up our Jupyter Notebook and get started. Run jupyter notebook on your command line, and this should either open up a browser window automatically or you can paste this URL into a browser window to get started. Here is our Jupyter Notebook interface, and we are in our current working directory where we'll write all of our code. Within our current working directory is a datasets folder. Here is where I be storing all of the datasets that we'll be using in the demos of this course. We'll primarily be using two datasets, the auto- mpg.csv and exams.csv. Go back one level up and use the New option to create a new Python notebook to get started coding. This notebook will use the Python 3 kernel. Here is our new untitled notebook on a new browser tab. Click on the notebook name in order to rename this notebook. I'm going to call this ExploringTheAutomobileMpgDataset.

Exploring the Automobile Mpg Dataset
If there are Python libraries that you need to install along the way as you write your code, you can do that using a pip install within the Jupyter Notebook as well. Make sure you proceed your pip command with the exclamation point. Write code in each cell of your notebook and hit Shift+Enter to execute your code. Now let's go ahead and set up imports for the other libraries and modules that we'll need. We have the sklearn library for scikit-learn packages, we'll use pandas and NumPy to work with our data. We'll use Matplotlib and Seaborn to visualize our data, and we'll use the datetime library to perform a little date manipulation in our code. In case you don't have any of these libraries installed, say pandas, Matplotlib or Seaborn, you can get them using a simple pip install. Let's quickly print out the current version of sklearn that we're working with, this is version 0.20 .2. The version of NumPy that I have is 1.16 .1, and for pandas is 0.23 .4. For most of the demos in this course, we'll be working with the automobile dataset that is available here at kaggle.com. This is a dataset that contains a number of different automobile features, which we use to predict how many miles that automobile runs per gallon of fuel. This dataset is available as a CSV file in the datasets folder under my current working directory. I'll use pd.read_csv to read in this file. If you want to view a sample of records in your data frame so that you can explore the dataset, you can call the df.sample function. The parameter 5 indicates that five records should be displayed. And here are five records chosen at random from our dataset. The columns at the very right make up the features of our machine learning model. The regression models that we're going to build will use these columns in order to make predictions about the miles per gallon for that car. There are features such as the number of cylinders the car has, the displacement of the car from the bottom, the horsepower, the weight, the acceleration, model, year, the origin of the car, and the name of the car. The first column off to the left, the mpg column, gives us the miles per gallon for that particular car, and this is what we'll try and predict using regression. The shape variable for any dataset gives us how many records are in the dataset and how many columns. So we have 398 records and nine columns of data. These nine columns include eight columns of features and one column that forms our machine learning target, the value we are trying to predict, the mpg. Now, datasets that we work with in the real world often contain missing fields or values, and these records need to be handled and cleaned in some way. This is part of the data wrangling or preprocessing that will apply to our data. Now this particular dataset contains question marks in place of missing fields; we'll replace all of those question marks with NaNs, or not a numbers. Call the automobile_df.replace function in order to perform this replacement. And once you have NaNs in place of missing values, it's very easy to clean your data frame. The drop any function on your pandas DataFrame will simply drop all of those records which have any fields missing. And if you take a look at the shape of your data frame now, you see that we have 392 records. We originally had 398 records, and now it's 392. Six records had missing fields, they were dropped. While we are building up the features for our linear regression model, it's pretty clear that the origin of the car and the name of the car has no impact on its mileage. This is something that we can determine just by a cursory look at the columns in our data frame, so go ahead and drop the origin and car name columns in place. These features, we know by using our common sense and logic, have no predictive powers. I'm going to call automobile_df.sample to sample five records from our data frames. And here are the features that we're going to work with: cylinders, displacement, horsepower, weight, acceleration, and model year, and the miles per gallon is our target, what you're going to try and predict. Now this dataset is from the '90s, and you can see that all of the model years are basically 1973, 78, 82, and so on. Now the model year by itself is just an object. Let's make this useful by converting this to be the age of the car. It's quite possible that we don't know for sure that the age of the car might have some impact on its mileage. Before we get to the age, let's convert the year to its full form, 1973, 1980, and so on, so I'm going to prepend the string 19 to the model year. So 19 + model year as string, will give us the resultant model year. Assign this new format to the model year column and let's sample our data frame and take a look at the result. The model year now has the full year, 1982, 1972, and so on. Now with this, we can calculate how old this particular car is. You can choose any reference date to calculate the age, as long as it's later than the last year that the car was made. In order to keep things simple, we'll calculate each field by subtracting from the current year. I'll use the datetime library to access the current year we're at; this year will be in numeric form. And I'll convert the data in the model year column to numeric form by calling pd.to_numeric. The result will be a number that will represent the age of a particular car. Go ahead and drop the original model year field, we no longer needed because we have the age column. Let's view a sample of this data frame. Once again, you can see we now have each column which tells you how old this particular car is. The absolute values for these ages don't really matter so much. It is their relative values that are more significant. If a car is older than another, it's possible that its mileage goes down. If you're building and training a machine learning model, all of the inputs to your model need to be numeric. Take a look at the data types of the different columns. You'll find that all of them are numeric except for one, that is the horsepower column. The horsepower is a numeric field, but its data type in our data frame is object. We need to fix this. This is very easily done using pandas. Simply call pd.to_numeric to convert horsepower to a numeric field and assign it to the horsepower column once again. Let's now call describe on our dataset in order to get a few statistical bits of information about all of our numerical features. You can see that all of the features in our dataset are now numeric. We have mean value, standard deviations, and the different percentiles displayed here. The describe function in pandas is an easy way for you to get a quick feel for your numeric data.

Visualizing Relationships and Correlations in Features
Understanding the features of our dataset and what we're trying to predict is the first step. The next step is to explore the data using visualizations. I'm going to use Matplotlib to plot a few scatter plots in order to understand the pairwise relationships that exists in my data. This is my first scatter plot; here I'm going to plot age versus the automobile's miles per gallon. We thought it might be possible that the older car is, the lower its mileage. Let's see if that's true using our visualization. And you can see that there is a definite downward trend here. Now this doesn't necessarily mean that a relationship does exist that needs more statistical analysis, but this visualization seems to tell us that older cars have lower mileage. Let's plot another scatter plot here. This time we'll try and see whether the acceleration of a particular car has any impact on mileage. Here is our resulting scatter plot, and you can see with acceleration on the x axis and miles per gallon on the y axis, there's a definite upward slope to the scatter plot. So maybe there is a relationship here. I'm curious about another one of our input features, that is the weight of the car. Does the weight of the automobile have any significant impact on its mileage? Maybe this scatter plot will give us some information. And yes, definitely there is a downward trend here. It seems like greater the weight of the car, lower its mileage, which makes sense to us intuitively. What about how the car is positioned relative to the ground, the displacement of the car versus mileage, is there any relationship? And once again, the visualization seems to say yes. It seems like greater the displacement of the car off the ground, lower the miles per gallon it travels. This pairwise exploration of variables really helps us cement our understanding of the underlying dataset. What about horsepower, does it affect the miles per gallon? Yes, indeed, it does. Remember that visualizations are not perfect. They are great at getting you a feel for your data. In order to assess the significance of these relationships, you'll need additional statistical analysis. Let's consider one last visualization here, cylinders versus mpg. And this scatter plot definitely seems to be a little harder to pass as compared with others. Cars with four cylinders overall seem to have the best miles per gallon. When you train your machine learning model, you feed it features that you think are significant. Now it's quite possible that your features themselves have interrelationships or correlations with one another. Correlations is a statistical measure that tells you whether and how strongly pairs of variables are related. Data frames offer this nifty little core function that will list out pairwise correlations between every pair of variables in your dataset. Correlation values are floating point numbers between -1 and 1. One implies a perfect positive correlation between two variables. You can see here that every variable is perfectly positively correlated with itself. Positive correlation implies that two variables move together in the same direction. A negative correlation implies that the two variables move in different directions. The raw correlation numbers tell us that acceleration is positively correlated with the mileage per gallon. You can also see that weight is negatively correlated with miles per gallon. In fact, weight is highly negatively correlated, - 0.83. Viewing correlations with the raw numbers is hard, which is why we use a visualization technique called the heatmap in order to view correlations in our data. When we pass in annot is equal to True to the heatmap in Seaborn, it will print out the actual correlation number along with the color-coded grid. And this is what a heatmap looks like. Lighter colors tending towards cream denote positive correlation, darker colors tending towards black denote negative correlation. This value of - 0.58 is in the mpg row and the age column. This shows that the miles per gallon seems very negatively correlated with the age of the car. We've done a bunch of preprocessing on our dataset, we've also viewed the relationships in our data. Now let's take this updated data frame and shuffle it so that we feed and shuffle data to our machine learning models. I'll use the sample function on our data frame to shuffle my dataset, I'm keeping all of the original samples, frac is equal to 1, and I'm resetting the indices. Drop is equal to True, passed into reset_index will drop the original index values that existed in our data frame. Here is our shuffled and cleaned up data frame. Now, shuffling data before feeding into an ML model is important so that our model doesn't inadvertently pick up patterns that do not exist. Machine learning models, typically neural networks, not really the kind of models that we are working with here in this course, are likely to pick up patterns even in the order you feed in data, so it's important that your data be shuffled. I'm going to save my shuffled and cleaned up dataset to a new CSV file, auto-mpg- processed.csv. This is the CSV file that I'll use to build my regression models. The to_csv function on the data frame, we'll see about this file, you can run an ls command using the exclamation at the beginning in order to confirm that this file has been safely written out. Yes, indeed, there it is. We're now good to go ahead with building our model.

Mitigating Risks in Simple and Multiple Regression
When you're working with regression, you should be aware of the risks that are involved. Let's talk about the risks in multiple regression. The regression that we've seen so far has always expressed data in a two-dimensional plane. When you have data in two dimensions, what you're performing is simple regression. When you have data in multiple dimensions, greater than two, that's when you perform multiple regressions. Multiple causes or explanatory variable, one dependent variable. For simple regression, there are risks that exist, but this can usually be mitigated by analyzing the R squared, or the measure of fit, of your line and the residuals of the fitted line. But in the case of multiple regression, the risks are far more complicated and need to be understood well. You need to be able to interpret the regression statistics that you get along with your regression analysis. Let's talk about the risks involved in simple regression first. They're easy to understand. It's possible that you're regressing two values when there is no cause-effect relationship between them. Regression is on a completely unrelated data series. The second risk that you might have to watch out for in simple regression is that your relationship itself might be mis-specified. It's possible that the relationship between variables is not linear in nature, it's an exponential or polynomial relationship, and you're trying to fit a linear model. This is a risk. And the third risk is that often incomplete relationship. You know that multiple causes exist for the dependent variable, but we have captured just one. We have just one cause when there are multiple causes. You'll find that the R squared that is a measure of fit of your underlying model is enough to help you diagnose these risks. When there is no cause-effect relationship, you'll find that your linear model has a low R squared and the plot of the XY data has no underlying pattern. For a mis-specified relationship, you might have a high R squared, but you might find that the residuals are not independent of one another. And if you have an incomplete relationship, you will find that your fitted line has a low R squared and the residuals are not independent of X, where X is the cause or the explanation variable. Once you've diagnosed the right issue, there are different techniques that you can use to mitigate risks in simple regression. If there is no cause-effect relationship, you made the wrong choice of X and Y, go back to the drawing board and figure out better data to work with. If the relationship is mis-specified, you might want to transform X and Y, convert to logs or returns. Convert them to a different form before you use linear regression. And if simple regression represents an incomplete relationship, you add X variables, you move on to multiple regression. And this gives me a nice lead-in to discuss the risks associated with multiple regression. The big new risk with multiple regression is that of multicolinearity, X variables which all contain the same information. Remember with multiple regression, you have multiple X variables, X1 all the way through to Xk. Now it's possible the two or more variables contain the same information. Let's consider two variables, X1 and Xk. These are the variables that we use in multiple regression, and you can see from this visualization here that these are highly correlated explanatory variables. You can see that the value of X1 moves with Xk and in the same direction. Multicollinearity has been detected. This essentially means that X1 and Xk contain the same information, the information that they hold are not different from one another. Now if you have a relationship between X1 and Xk expressed like how you see here on screen, it's pretty clear here that X1 and Xk are uncorrelated explanatory variables, they contain different bits of information and they are both useful in our regression analysis. Working with X variables that are correlated kills regression's usefulness; regression is no longer useful to explain the variance in your underlying data, the R squared, as well as the regression coefficients that you get from your model are not very reliable. Regression coefficients are basically the intercept and the slope of your linear regression line. Regression analysis that you've performed with multicolinear data is also not very useful for making predictions. The regression model will perform poorly on new data that it hasn't seen before, out of sample data. So how do you mitigate this risk of multicolinearity? That is prevention and cure. First, apply your common sense to get a big picture understanding of the data, think deeply about each X variable and eliminate those that are closely related and dig down to underlying causes. Does a cause-effect relationship exist? These two variables convey the same information? These are the questions you need to ask. You also need to get your nuts and bolts right. When you set up your regression problem, make sure you standardize the variables that you're working with by subtracting the mean and dividing by the standard deviation so that all of your values are expressed in terms of standard deviations from the mean. There are other factors as well. Instead of looking at the R squared measure of your linear model, you look at the adjusted R squared, and we'll talked about that a little more in just a bit. And finally, as you get a little more advanced in your machine learning study, you might want to perform dimensionality reduction on your data to extract the significant features in your data. These involve techniques such as factor analysis and principal components analysis.

R-squared and Adjusted R-squared
Let's say you've successfully performed regression analysis, how do you know that the model that you fit is a good one? We spoke about R squared, the most common and popular metric for evaluating your regression analysis. R squared is a measure of how well your regression model has captured the variance in the underlying data, how well your straight-line relationship fits the underlying data. Now, R squared is expressed between 0 and 100%, or between 0 and 1. Unfortunately, there is a problem with R squared in that as you add new X variables for multiple regression, you will find that the value of R squared constantly increases, leading you to believe that your model is improving when it may not be. This adding of X variables where R squared increases can lead to overfitting your model on your data, and you know that overfitted models are not really great for predictive analytics. So you can't completely rely on R squared when you're performing multiple regression, you need another metric, and that's where the adjusted R squared comes in. The adjusted R squared is preferred for evaluating multiple regression. Adjusted R squared is derived from R squared, and it basically adds in a penalty for adding irrelevant variables to your regression model. It uses statistical testing to determine whether new X variables that you've added to your multiple regression are relevant or not. And adjusted R squared increases if irrelevant variables are deleted.

Regression with Categorical Variables
There are certain things to understand when you perform regression analysis using categorical variables. Categorical variables are those that take on discrete values, true or false, yes or no, 0, 1 or 2. Let's understand the nuances of regression analysis with categorical values using a simple regression example. Here is the regression equation, y is equal to A + Bx. Y here is the height of an individual, a child, and x is the average height of that child's parents. Now, if you happen to plot x versus y for males and females, you might find a distinct pattern there. You might find that males are on average taller than females in your dataset. If you try to fit a simple regression line here, this line might represent the relationship in this manner, you can see that this line isn't really a great fit. The regression is far from all of the individual points. If you have separate regression lines for males and females, you might find that they fit a lot better. Let's say you have one regression line for males, y is equal A1 plus Boolean, that is a great fit for the males in our dataset. You might have another regression line for females, y is equal to A2 + Bx. What we have here is two regression lines with different intercepts, A1 and A2, but the same slope. So the slope for both of the lines is exactly the same, that is the dependence on X, but the intercepts are different. Here you can see the regression lines for males and females, both of which are great fits on our data. Let's go ahead and add a dummy variable to get a combined regression line. This combined regression line on the form, y is equal to A1 + A2 - A1 multiplied by D, our dummy variable, + Bx. B here is the new dummy variable that we introduced. B is equal to 0 for males and 1 for females. When B is equal to 0, you can see that our combined regression line reduces to the regression line for males. The result is A1 + Bx, which is the same as the regression line for males. When D is set to 1 for females, once again, our combined regression line reduces to the regression line for females, A2 + Bx. Now this very neat use of the dummy variable allows us to have one combined regression line representing both males and females, that is the categories in our data. Now because we had exactly two categories in our data, males and females, we added one dummy variable, but this exact same idea can be extended with categories with multiple discrete values. So if you have categorical data with k categories, such as days of the week that has 7 categories, months of the year has 12 categories, you need to set up k-1 dummy variables, otherwise this results in multicolinearity. When you use a library such as scikit-learn to build your regression model, you don't have to do any of this manually. Scikit-learn takes care of this for you under the hood. However, you should be aware of these nuances and these details that go on under the hood in case you need to debug your linear regression. So if you're performing regression analysis with categorical variables, you need to have dummy variables. So if you have binary categories 0 or 1, just one dummy variable is enough. If you have categorical variables with a finite set of values where the set of values is equal to k, you'll have k-1 dummy variables. For the most part, you won't have to deal manually with any of this, but you should be aware of this so that if certain edge cases arise where you run into multicolinearity, you know how to deal with it.

Module Summary
And with this, we come to the very end of this module where we understood linear regression as a machine learning problem. We saw that linear regression is used to express a straight-line relationship in your data that can then be used for predictive analytics. Linear regression can easily be extended to multiple regression where you consider multiple explanatory variables. We discussed the idea of mean square error or least square error as the loss function of your regression model, and we saw how we could interpret the results of a regression analysis. We discussed the risks associated with simple regression and how we could mitigate them, and we discussed multicolinearity, the major risk in multiple regression. We also covered in detail the use of R squared as a measure of fit for evaluating our regression models, and we discussed the advantages of adjusted R squared for multiple regression. On the demo front, we got installed and set up with the scikit-learn library and explored the data that we're going to work with. The next module is all hands-on. We'll perform simple regression, multiple regression, and regression using categorical variables.

Building a Simple Linear Model
Module Overview
Hi, and welcome to this module on Building a Simple Linear Model using the scikit-learn library. The last module we spent on understanding the basic concepts underlying regression as a machine learning technique. In this module, we'll focus on hands-on demos. We'll see how we can build a linear module in scikit-learn with just a single feature or a single X variable. This is simple regression. We'll then extend this technique to multiple regression where we have many features or X variables. We'll use the LinearRegression estimator object to fit on our training data and use it for prediction as well. It's the same estimator objected whether you're performing simple regression or multiple regression. And finally, we'll also see an example of performing linear regression using data that has categorical values. You'll see that we don't really need to do anything special to deal with categorical values. The scikit-learn library takes care of everything for us under the hood.

Simple Linear Regression
In this demo, we'll perform simple linear regression using the LinearRegression estimator object. We'll see how we can build a linear model in scikit-learn. We'll write the code for this linear regression model in this IPython Notebook, SimpleRegression_ NumericValues_AutomobileMpg. Set up the import statements for the libraries and modules we'll need. We'll use pandas, Matplotlib, and NumPy. I'll add in the import statements for other scikit-learn specific modules as we need them. Let's read in our auto MPG dataset that we've already cleaned and processed earlier. This is in the file auto-mpg- processed.csv in the datasets folder under the current working directory. Here is what the dataset looks like. The features are cylinders, displacement, horsepower, weight, acceleration, and age, and we'll use these features in a linear regression model in order to predict the mileage of the car. When you are building and training a machine learning model, how do you know that the model that you've built is a good one? Well, you'll evaluate your model on test data. Test data are basically a holdout from your training dataset. These are instances your model hasn't seen before, and you'll see how well your model predicts using those instances. Scikit-learn offers a useful train_test_split function in order to split your data into training and test sets. Let's first perform a linear regression using just one feature; that is, we'll see how the horsepower of a particular car can be used to predict its mileage. So your X variable is going to be just horsepower, and the Y variable is going to be MPG. The Y variables are what we'll try and predict using our machine learning model. Let's split our data into training and test sets. It's pretty common to use 20% of your data as test data used to measure and evaluate the model that you build using your training data, which is 80% of your dataset. We've already shuffled our dataset earlier; however, you should know that the train test split function in scikit-learn automatically shuffles your data as well. Let's take a look at a sample from our training dataset. We've used just one feature here, we have just one column for horsepower. Scikit-learn offers us high-level estimator objects that we can use to build and train our machine learning model. In order to perform linear regression, we'll use the LinearRegression estimator object. Import this object and let's instantiate a linear model. The features that you feed into your machine learning model are numeric and typically when you're working with numbers, your ML model performs far better if you normalize your data. If you pass in the parameter normalize is equal to True to your LinearRegression estimator object, this will scale all your numeric features to be between 0 and 1. For a simple model, such as the ones that we'll build in this particular course, you'll find that normalizing your dataset may or may not make a difference, but for more complex models in the real world, normalizing your numeric data is a standard preprocessing technique for machine learning. The fit function on an estimator object is what you call to train your machine learning model. This fits the linear model on the training data, so it trains the linear model using our single feature, the horsepower, and it uses the target values to adjust the model parameters. A way to measure how well your model has performed on the training data is to score your model using the R squared score. The score function on your linear regression model will return the R square value for your training data. As we discussed, this R square value is a measure of how well our linear model captures the underlying variation in our training data. And you can see here that with just a single feature, horsepower, our model has an R square of 59%. It isn't great, but it isn't terrible either. Now that we have a fully trained model built using a single feature, let's use this model for prediction. Call linear_model.predict and pass in our test data, as in only X values are the features to predict. And here are our predictions, saved in y_pred. A way to objectively measure how well your linear model performed on instances it hasn't seen before is to calculate the R square score on your test data. The sklearn.metrics namespace offers a number of useful metrics to use with your ML models. Import the r2_score function here from the sklearn.metrics namespace, and use that to score how your model performed on the test data. Pass in the predicted values from your model, and compare them with the actual values in your test data. And with just one feature, our linear regression model has an R squared score of 65%. Compare that with the R square score of the training data. The R square on test data is better than on training data, which means that our model is a good, robust model, not overfitted on the training data. An overfitted model is one that does well on the training data, but does poorly when used for prediction or on test data. Let's use a little visualization to see how well our linear model fits on the underlying data. I'll first plot a scatterplot of horsepower versus miles per gallon. This scatterplot represents the test dataset and the actual Y values. I'll then plot a line representing the horsepower and predictions from our linear model in the red color. And here is what the resulting visualization looks like. The scatterplot represents the test data, the red line represents the predicted values from our model. This is our linear model. Let's build one more linear model. This time we'll use a different feature. The steps involved are exactly the same as what we saw just a little bit earlier. The feature that we'll use this time to train our model is the age of the car. The features from our X variables and our Y values are the target, what we want to predict, miles per gallon. Split our dataset into training and test data, initialize a LinearRegression estimator object, make sure you normalize your numeric features, and call fit on the training data. Once we have a fully trained linear regression model, print out the R square score for this model on the training data, use this model for prediction on the test data, and print out the R square score on the test data as well. And here is how the two scores compare when we use age as our feature. The training R square score is 36%, and the test score is 19%, so this is a pretty poor model. So, age by itself is not really a good predictor for the car's mileage, and this will be born out using our visualization as well. Plot a scatter plot of X versus actual Y and X versus Y predicted, and here is what the result looks like. You can see that the line that we've drawn here really doesn't capture the underlying variation in the data well, which is why this model has a low R square score. The points are too scattered, too far apart, the line really doesn't represent them well.

Linear Regression with Multiple Features
We'll continue working in the same notebook as before, SimpleRegression_NumericValues_AutomobileMpg. This time, instead of using a single feature to train our model, we'll use more than one feature. Create our X variables that we'll use to train our model using displacement, horsepower, and weight. We'll use these three features to predict the miles per gallon for the cars in our dataset. MPG is our Y variable assigned to the Y data frame. With our data all set up, let's call the train_test_split function to split into training data that we'll use to build our model, and test data that we'll use to measure our model. We'll continue to work with the LinearRegression estimator object, and we'll normalize all of the numeric features that we pass in to train our model. Call the fit function on your estimator object to start the training process, and pass in the training data and the corresponding training Y values. Once the model has completed training, call the linear_model.score function to calculate the R square score on the training data. You can see from this R square score here that the additional features that we've used to train our model have real predictive power. Our R square has improved, it's now 68.7 %. Earlier when we used just a single feature, horsepower, our R square was around 59% on the training data, so this is a definite improvement. A linear model assumes a linear relationship between your input features and the output that you're trying to predict, and this linear relationship can be represented as Y is equal to WX + B, where W is the weight, or the coefficients that you use to multiply your X variables, the features. W is also referred to as the model weight or model parameters. You can use the linear model instance in order to get the coefficients for your X variables or the predictors. Get the predictors from your x_train data frame, these are the columns of your data frame, and we'll instantiate a pandas series with a coefficient for each predictor. And let's print out these coefficients and see what they are. The coefficients of your linear model for horsepower, displacement, and weight are all negative. This indicates that as the values for these features increase, the mileage of the car tends to go down. The R square on the training data for this model with additional features was much better. Let's use this model for prediction and store the predicted values in y_pred. Let's now calculate the R square score for the test data. And you can see that there is a significant improvement here as well. The R square score on test data is almost 76%. Once again, the R square calculated on test data is better than the R square on training data, indicating that it's a robust model. The overall higher R square that we get with three features instead of one means that this model has better predictive power. It's always fun to visualize our results. Let's plot the predicted values versus actual values from our dataset using a line chart, and see how closely they track one another. And here's what the result looks like. The values predicted by our model are in blue, and actual values are in orange. You can see that these two lines track each other pretty closely. Well, we added more features, we got a better model. What if we add even more features? Displacement, horsepower, and weight are the features that we're using currently. There are still some features in our dataset that we haven't used. I'm going to add those in here as well. I'm going to include acceleration, as well as the number of cylinders. There is still another feature that you could use, you could include the age of the car as well. That's something you can tweak if you want to. For now, I'm going to go over these five features to train our machine learning model. We already have our code written. Hit Shift+Enter in order to execute the code in all of these cells. We'll instantiate a new LinearRegression estimator object and fit on this new training data. Once this new model has been trained, let's calculate its R squared. Let's see what kind of a model it is. Now, the original R square that we got earlier with three features was around 69%. Hit Shift+Enter to calculate the new R squared, and you might say wow, it's 73%, this is definitely a better model. Let's go ahead and hit Shift+Enter once again to get the coefficients of all of the features that we've included. We get five coefficients corresponding to the five features that we used to train our model. Our training R square was really high, seeming to indicate a better model. I'm all agog now to see how well our model performs on test data, data it hasn't seen before. Use the linear model to predict on our x_test dataset, save the predictions in y_pred, and let's calculate the r2_score for our test data. Our previous model, with just three features, had an R square score of 75.9 %, almost 76%. Let's hit Shift+Enter and see how this model does, and this is where we come back to Earth. The R square on test data is 60.9 %, almost as low as the R square that we got when we trained a model using just a single feature, the horsepower. So a lesson learned here. More features do not necessarily make a better model. What we just performed is what is sometimes called kitchen sink regression where we throw all of the features that we have into our model. Kitchen sink regression does not necessarily perform well because all of our model features may not have good predictive power. And this is why machine learning is all about training and evaluating different models to see what works well in the real world.

Standardizing Numeric Data
In this demo, we'll perform a linear regression for prediction once again, but this time we'll use a slightly different dataset with different kinds of features. This is a dataset that contains categorical values, and we'll see how we can convert these to numeric form so that we can use them to train our machine learning model. In this demo, we'll build our regression model in a new Jupyter Notebook called SimpleRegression_CategoricalValues_ExamScores. That's the dataset we're going to use, exam scores dataset. Let's import the libraries that we need. We'll use pandas, as well as Matplotlib, and we'll use pandas to read in our data. This is a very simple toy dataset present in the exams.csv file. The original source for this dataset is this URL that you see onscreen here, at roycekimmons.com. Once you've read in the dataset into a data frame, call the sample function and let's explore what we just loaded. This dataset has just 100 records, which is sufficient for the purposes of our demo, and it contains information about different students. You can see that some of this information is personal. We know the gender of the student, the race or ethnicity he or she belongs to, the parental level of education, whether the student has standard or subsidized lunch, and whether the student has joined a test preparation course. You can see that these personal details are categorical, or discrete values. They're not numeric either, they're represented as string values in our dataset. We also have a few features describing the scores for that student in math, reading, as well as writing. All of these scores are out of a hundred. These are the only numeric features in our dataset with continuous values. The describe function on a pandas DataFrame will give us brief statistics about all of the numeric values in our data frame. This dataset has a total of 100 records for 100 students. You can see the average scores for these students in math, reading, and writing. You can see that math scores are a little lower than their reading and writing scores. The standard deviation of these scores, how these scores vary across students, is also different. This dataset is interesting because the data needs a lot of preprocessing before we can feed it into a linear model. And scikit-learn makes it very easy for you to preprocess data using the preprocessing module from sklearn. Go ahead and import preprocessing. We're going to use the preprocessing.scale function to standardize all of the scores in our dataset. Standardizing a dataset means that these column values will now have 0 mean and unit variance. We'll have a variance of 1. Standardizing values is extremely useful because it gives you an easy way to compare values which are part of different distributions. Standardization is also a common preprocessing technique for machine learning algorithms to build more robust models. We call preprocessing.scale to standardize the math score, reading score, and writing score of all of our students. Standardization is done by subtracting the mean, or average, value of a column of values from each value in that column and dividing the number by the standard deviation of the column. If you sample your data frame now with the standardized values for the different scores, you can see that the scores are now very small numbers. Negative scores are those which are below the mean, and positive scores are those that are above the mean. Standardizing a dataset allows you to see this at a single glance. You don't need to know the actual numbers, actual mean values, nothing. You can see that this particular student has been doing pretty poorly in her exams. All of her scores are more than one standard deviation below the mean. Let's call describe on our data frame once again. Our data frame now contains standardized values for scores. You can see that mean values are very, very close to 0, and the standard deviations for all three scores are very, very close to 1. This is what standardization has done.

Label Encoding and One-hot Encoding Categorical Data
What's interesting about this dataset is the fact that many of its columns contain discrete or categorical values, such as the parental level of education column. Call the unique function in order to see the unique values represented in this column. These are the differing levels of education for the parents of these students. All of the students belong to the same grade. For this particular field containing categorical data, you know that there is an intrinsic order in the level of education. Some high school, then comes high school, then some college, then associate's, the bachelor's, then master's degree. Categorical values have to be converted to numeric form before they can be used in your ML model, and when there is an ordering associated with your categories, you should use the preprocessing.LabelEncoder object in scikit-learn to convert categorical values to integer values to use in our ML algorithm. Instantiate the LabelEncoder object, and call fit on the parent_level_of_education array. The result will be an ordered label encoding of these categories. Every category will be represented by an integer value. And these integers can then be fed into our ML model for training. Let's transform our parental level of education column in our data frame to these unique integer labels by calling label_encoding.transform. Our parental level of education column will now contain integer values representing the different levels of education. Zero represents some high school, 1 represents high school, and so on. Label_encoding.classes_ gives you the classes that were encoded as integers. These are the various levels of education for the parents of students. If you have values in your dataset that are categorical in that they are discrete values, but there is no intrinsic ordering between these values, you can convert these categorical values to numeric representation using one-hot encoding. For example, the race or ethnicity that a particular student belongs to is just a category. There is no ordering between these races. The pd.get_dummies function will allow us to represent these categories for students in numeric form using one-hot encoding. The pd.get_dummies function will replace the original race/ethnicity column with a column representing each race. Race and ethnicity are represented by categories group A, group B, all the way up to group E. And you can see that there is a column associated with each of these groups after we've one-hot encoded this information. A student who belongs to group E will have a 1 in that particular column, all other columns will be 0s. A student belonging to group B will have a 1 in that column, other columns will be 0s. This is how one-hot encoding of categorical data works. We'll now perform the same one-hot encoding for other categorical values in this dataset. The columns for gender, lunch, and test preparation course. I'll invoke pd.get_dummies on all of these columns. And once we've done this, let's take a look at what the resulting data looks like. You can see that this data has many more columns, as all of our categorical values have been one-hot encoded. Here are the one-hot encoded columns for gender_female and gender_male. Here are the columns for students who have free/reduced lunch or standard lunch. And here is the one-hot encoded column showing us whether the student enrolled for a test preparation course or not. Now that we have our data all in numeric form, including the categorical values, let's set up our training data and our test data. We'll try and predict the math score for a particular student using the other features in the dataset. So we'll use the personal details for every student, along with their reading and writing scores to predict their math scores. The x variables, or the features that we'll use for training, are all columns other than the math score. So drop the math score and assign the rest to x. The y variables are the target values that we're going to predict using linear regression is the math score for every student. Call the train_test_split function in order to use 80% of the data to train our model and 20% to test our model. This is a toy dataset. Our training data has just 80 records, and our test data has 20 records. And the corresponding thing is true for the y labels for the values as well.

Linear Regression and the Dummy Trap
With our dataset all set up, we are now ready to fit a linear model on our data. From kslearn.linear_model, import a LinearRegression estimator. Instantiate the LinearRegression estimator object, and we explicitly pass in the parameter fit_intercept is equal to True. Now we've used one-hot encoding for the features in our dataset, and we have set fit_intercept to True, now this particular setup might cause us to encounter what is known as the dummy variable trap. The dummy variable trap occurs when there is perfect collinearity between two variables that we've used in our model. This trap is encountered if we fit an intercept on our linear model and we use all of the columns from our one-hot encoded variables. Let's try training our model with these particular parameters, fit_intercept=True and one-hot encoding with all of the columns intact in our features, and see what happens. The model trains with no errors, things seem to be fine here. Let's calculate the score of this model. Here is the R square, and the R square is 88%. This is very good. It's a simple dataset, which is why we have this high R squared. Let's use this model for prediction on our test data. Call linear_model.predict on x_test, and let's calculate the testing R square score as well. And the testing score is 85%. Once again, quite good. Let's now run the same model, the linear regression model, on the same data. This time we'll set fit_intercept to False. When we've used all of the columns in our one-hot encoded labels, fit_intercept should be False. Once the training of this model is complete, calculate the training score. You'll see that it's once again 88.8 %, the same as before. Use this model for prediction and calculate the test R square on the test data as well. Once again, it's 85%. Now, if you've been following along closely, you might have realized that our training as well as test R squares with fit_intercept=True, as well as False, are exactly the same. Why is this? So when we previously had set fit_intercept to True, we said that we might encounter the dummy variable trap. But that is clearly not the case. With fit_intercept=False, we get the same results as with fit_intercept=True. This dummy variable trap is often encountered in the real world, which is why the scikit-learn LinearRegressor estimator object accounts for this intercept when you use one-hot encoding in your features. So whether you set fit_intercept to True or False, it does not matter with the LinearRegression estimator object. If you've used one-hot encoding for your features, the LinearRegression object will make sure that fit_intercept is False under the hood so that you don't fall into the dummy variable trap. So this is a good thing when you're using scikit-learn's LinearRegression object. Not all of the estimator objects in scikit-learn account for this though, so you have to watch out and be a little careful. Now that we have predicted values from our model, let's set up a data frame with the actual versus predicted values and take a look at some actual predictions. Because we standardized the scores when we fed them into our model, the output scores are also in the standardized form. The actual and the predicted scores seem to be pretty close. The high R square should tell us that this is a good model. Let's plot line charts of actual versus predicted scores. You can see that they're very close together. Let's try building this model with a few different features. Let's try predicting the math score for each student without using any of the other scores. The only thing that we change here are our x variables. Drop the math score, writing score, and reading score from x. We'll only use the student's personal details to predict his or her math score. Split up the data into training and test, instantiate and train a linear regression model, calculate the R square score for training, as well as test data. And you can see that for this particular model our R-square values are really low. That's because it is the other scores that have higher predictive power for the math scores. Let's try this once again with a little variation. We'll try and predict the math score using only the reading score along with other features. We won't use the writing score. So reading score alone. Drop the math score and the writing score from our x variables, and go ahead, split up the data, train the model, and print out the R squares. And you can see that on this simple toy dataset, our R square values for training, as well as test, are pretty high when we use just the reading scores along with other features to predict the math score for a student.

Module Summary
And this demo brings us to the very end of this module where we built our first few regression models in scikit-learn. We first built a very simple linear model with just a single feature. This is simple regression. We then moved on to performing multiple regression with multiple x values or features. If you have a dataset and you simply throw in all of the features that you have in your training data, that is often referred to as kitchen sink regression, and that may not always give you the best results. The original dataset that we worked with did not have any categorical values, which is why we used a different dataset to perform linear regression using data with discrete or categorical values. We'd spoken about using dummy variables to deal with categorical values, and the scikit-learn library takes care of all of this for us. We didn't have to deal with any of the grungy details. In the next module, we'll talk about building models to mitigate the problem of overfitting on the training data. We'll talk about how we can build regularized regression models.

Building Regularized Regression Models
Module Overview
Hi, and welcome to this module on Building Regularized Regression Models using the scikit-learn library. We'll start off by discussing the various regression algorithms that are available in scikit-learn and when you would choose to use regression to solve your problems. We'll then discuss in some detail the problem of overfitting on the training data. An overfitted model performs well during training, but poorly in test usage in the real world. Mitigating overfitting is an important aspect of building ML models, and we'll talk about the bias-variance tradeoff in that context. We'll specifically discuss regularization as a technique to mitigate overfitting. Regularization involves the addition of a penalty function to the objective function of your regression model. And this is what forces the regression optimizer to keep the model simple. We'll then see how we can build and train models for lasso regression, ridge regression, and elastic net regression. We'll talk about the differences between these three, and we'll talk about how elastic net is simply a combination of the lasso and ridge models.

Overview of Regression Models in scikit-learn
Now that you know that you want to use regression for predictive analytics, the next step is to figure out the regression algorithm. Remember, your focus should always first be on defining the right problem to solve and then choosing the right estimator to solve it. The scikit-learn library has a whole host of estimator objects implementing different machine learning modules, and there are a huge variety just for regression. You don't need to worry about all of the details in this complex flowchart that you see here onscreen, let's focus on the part where you would actually choose to use regression to solve your problem. If you have a dataset with more than 50 samples and you're not predicting a category as your output, you want to predict a numeric quantity, that's when you choose to use regression. You're working with the scikit-learn library and you want to implement a regression algorithm. There are a few more choices or decisions that you need to make. If you have fewer than 100 case samples and only a few features in your underlying data matter, then you choose to go with the lasso or elastic net regression models. So far what you've seen is the simple linear regression model and lasso and elastic net are variations on these, and we'll discuss the details of this variation later on in this module. If you have a huge number of features that matter and you have less than 100 case samples, ridge regression or support vector regression is what you choose. If you use support vector regression with the linear kernel, that is a form of linear regression, or if that doesn't work for your data you might use support vector regression with a different kernel, or ensemble regressors. Ensemble regressors are when you use multiple regression models where every model individually may not be great, but you bring them together to get a more robust model. However, if the dataset that you're working with has more than a 100, 000 samples, the SGD regressor, or the stochastic gradient descent regressor, should be your choice. I understand that all of these are just terms and you don't know what they mean, but that's exactly what we'll cover in this module and the next one.

Overfitting and Regularization
Before we move on to starting and building regularized regression models, we'll first discuss overfitting a model on the training data and regularization to mitigate overfitting. Let's talk about connecting the dots using the data in two dimensions. Here is a challenge for you. Find the best curve through these points. A good question for you to ask right now is what is the definition of best? A curve is said to have a good fit if the distances of the points from the curve are small, and this is where we began our study of linear regression. Now, it's also possible for us to draw a pretty complex curve through these set of points, we can have the curve pass through every single point that exists in our dataset. Now based on our previous definition of a good fit, this curve is actually amazing, the distance of this curve from all of the data points is 0. But if we want to use this curve for predictive analytics, given a new set of points this curve might perform quite poorly. It's not really great for prediction. A well-fitting curve that's useless, well, that's not really what we want. The original points if you consider were the training data and the new points as the test data, you can say that this is a model that performs very well on the training data, but poorly in the real world on test data. It's pretty clear here that if you had a simpler model, such as a simple straight line, it might perform worse in training, but better with test data for prediction. And this kind of intuitive understanding of modeling relationships leads us to the definition of overfitting a model that does very well in training, it has a low training error, but poorly with real data, it has a very high test error. This is an example of an overfitted model. Overfitted models are an important risk when you are building ML models in the real world, which is why there are a number of techniques that you can use to mitigate or prevent overfitting. The first of these techniques, which is by far the most straightforward, is the idea of regularization. Regularization is a technique where you add a penalty to models that are very complex that are overfitted. Another technique to mitigate overfitting is the use of cross validation to build your model. You'll have distinct training and validation phases before you actually test your model on data it hasn't seen before. A third technique that you can use to mitigate overfitting applies only to neural network models, this is dropout where you intentionally turn off some neurons during the training phase. Of these three techniques, the one that we'll focus on in this module is the idea of regularization. Regularization is what you do to penalize complex models, models that track the training data very closely are highly complex because they are trying to capture a lot of information from the training data rather than getting a more general idea of the relationship. Regularization in models is implemented by adding an additional penalty to the objective function. Remember, the objective function for linear regression is minimizing the least square error. We'll add another term, the penalty term, and this penalty will be a function of the regression coefficients. Higher and more complex regression coefficients, higher the penalty and the model will serve to minimize this penalty. This forces the optimizer that is your linear model to keep things simple, use simpler coefficients, get a more general idea of the relationship rather than overfitting on the training data. You should remember, though, that when you regularize your model it reduces the variance error. This means that your model will be less sensitive to the training data, it won't try to fit the training data very closely, that is the variance error. On the other hand, regularization increases the bias of your model. Bias refers to the assumptions that you make about the underlying model in order to get a more general fit or to fit a simpler relationship. When you build any machine learning model, there is a tradeoff involved within variance and bias. High variance models will tend to have a low bias, and low variance models will tend to have a high bias. And you have to figure out where this tradeoff is right for your use case, your dataset.

Lasso, Ridge and Elastic Net Regression
There are three regression models that you can build and train which use regularization to mitigate overfitting on the training data. These are the lasso, ridge, and elastic net regression models. So how do these three regression models work? Let's start with lasso regression. This penalizes large regression coefficients. Ridge regression also penalizes large regression coefficients. And elastic net regression is simply a combination of lasso and ridge. The difference in these three models lies in the penalty function that is used to penalize these large coefficients, the penalty function is different in each of these three cases. So far, we've stayed strictly away from mathematical formulas, let's introduce a simple one now. This is the loss function for ordinary mean square error regression, that's what we've been performing so far. The objective function is to minimize this, y actual values - y predicted values, the whole square and the whole under the root, this is mean square error. So when we fit a regression model, we're trying to find the best regression coefficients, A and B, which define the best fit line of the form y is equal to A + Bx. Let's move from ordinary least squares regression to lasso regression, and here is where the objective function will change. We'll add a penalty to the this objective function. Here is the mathematical formula for the penalty that lasso regression adds to the objective function, alpha multiplied by modulo of A and modulo of B, where A and B are the regression coefficients and alpha is a hyperparameter. A and B here are the same regression coefficients as earlier, they still define the best fit line, but we've added an additional penalty which depends on the value of A and B, the absolute value. And we apply a weighing factor to this additional penalty using alpha, the hyperparameter. We'll discuss hyperparameters in more detail later. This penalty function that we use for lasso regression is the L1-Norm of your coefficients where L1-Norm is defined using this formula that you see here onscreen. It is the absolute value of all of the coefficients raised to the power 1, that's where L1 comes from. So going back to our lasso regression objective function, here we have alpha and the L-1 Norm of regression coefficients. Now, if you were to change the penalty function so that it's the L-2 Norm of regression coefficients, you'll get ridge regression. Observe that the penalty that we've added to the objective function is different, it is modulo of the regression coefficients squared, and this is the L-2 Norm where the L-2 Norm is defined as the modulo of the coefficients raised to the power 2. Now that we have a basic understanding of the mathematics behind lasso regression, here is a quick summary. We've added a penalty for large coefficients where the penalty term is the L-1 Norm of the coefficients, and this penalty is weighted by the hyperparameter alpha. A hyperparameter is simply a design parameter for your model, and this is something that you can tweak to change the design of your regression model. If you set alpha equal to 0 in lasso regression, what you essentially get is ordinary least square error or mean square error regression. If you set alpha to infinity, you're forcing very small coefficients to be equal to 0. As you can see, your model can change substantially when you tweak this value of alpha. This is called model selection by tuning alpha, the hyperparameter. And this alpha helps you eliminate unimportant features in your regression, giving you a more robust model. And why is this called lasso regression? Because lasso stands for least absolute shrinkage and selection operator. The math here is complex and it's not something that you need to know, you just need to have an intuitive understanding that there is no closed form analytical solution to lasso regression. It needs a numeric solution which you'll build using your scikit-learn estimators. We saw that the differences between lasso and ridge regression were in the penalty function, ridge regression uses the L-2 Norm of regression coefficients as the penalty. The basic use case for ridge regression is exactly the same as that of lasso, add a penalty for large coefficients, the penalty term is different, it's the L-2 Norm of coefficients, and the penalty is weighted using the hyperparameter alpha. However, as discussed earlier, you'll choose to use ridge regression over lasso if you have a large number of features that can be considered significant for your regression. Unlike lasso, ridge regression has a closed form analytical solution. Also, when you use ridge regression, your model's coefficients will not be forced to 0. This is why ridge regression might be preferable when you have a large number of features that matter. Ridge regression, unlike lasso, does not perform model selection by turning off certain features. The last kind of regularized regression model, elastic net regression, simply combines lasso and ridge. The penalty function here can include both the L-1 Norm of coefficients, as well as the L-2 Norm, and you can specify the weight that you want to give each of these penalty terms.

Defining Helper Functions to Build and Train Models and Compare Results
In this demo, we'll study a number of different regression models that we can build and train in scikit-learn. This demo will be a long one and will span the rest of this module and will intersperse conceptual explanations using slides for each model before we dive into its implementation in scikit-learn. We'll build our different regression models in a single Jupyter Notebook, MultipleRegressionModels_AutomobileMpg. We'll use the AutomobileMpg dataset for all of these models. I'll first set up the imports for all of the libraries and models that I plan to use in this demo, pandas, NumPy, Matplotlib, and the sklearn modules. At this point in time you should be familiar and comfortable with all of these. And here are the import statements for the estimator objects that we'll use to build the different kinds of regression models, linear regression, lasso, ridge, elastic net, to name a few. We'll build all of these models in the same notebook using a few helper functions that we'll set up first. I've turned off warnings here in this Jupyter Notebook, this is not really recommended when you're working in the real world, though. I've only done this so that the warnings don't interfere with the actual demo. I'll tell you where the warnings might occur and tell you what they are for. You can choose to leave warnings on when you're working in your notebook. Let's go ahead and use pandas to read in our dataset that has been cleaned and preprocessed earlier. This is in the auto-mpg- processed.csv file. Here is what the dataset looks like, it needs no explanation at this point in time. We'll use all of the other features, cylinders, displacement, horsepower, and so on, to predict the mileage for the cars. I'm going to instantiate a dictionary here called result_dict that will hold the training and test scores from the different models that we build and train. The keys will be meaningful names for the different models that we build and the values will be their training and test R squares. In this way, by simply doing the results stored in this dictionary, we'll be able to compare different models. I'm going to define a helper function here called build_model that will allow me to build and train the different regression models. This helper function takes in a number of input arguments; I'll discuss that in detail. The first argument here is the regression function. This is a function that takes in a training data and corresponding target values. This will instantiate a particular ML regression model, whether it's a linear regression model, a lasso model, a ridge or an elastic net model, anything. And this function will train the model on our training data. The name of y_col input argument specifies the column name in our data frame for the target values that we should use for training. The names_of_x_cols is a list of feature columns. These are the columns that we want to include as features when we train our model. The dataset is the original data frame that contains the features, as well as our target values. The test_frac specifies how much of our dataset we should hold out to evaluate or measure our model, that is the fraction of our data that will be used as test data. If you want the data to be preprocessed in some way, standardized or scaled before you feed it into your regression model, you can specify a preprocessed function. By default, it's set to None. Set show_plot_Y to True if you want to display a plot of actual versus predict Y values, and set show_plot_scatter to true if you want to see how your regression line fits on the training data. Extract from the dataset the features that you want to train your model into the variable X and extract the target value into Y. If you've specified a function used to preprocess your model, apply this preprocessing function to your X values. The preprocessed features are stored once again in the X variable. Use scikit-learn's train_test_split function to split up your dataset into training and test data. Once you have your training data, pass in the training data, as well as the corresponding labels to the regression function. The regression function is a wrapper that will instantiate a particular regression model and train on the dataset you've specified. The regression function will return the fully trained ML model, which you can then use for prediction, and store your predicted values in y_pred. You can then print out the R square values on the training data, as well as the test data for your model. If you've invoked the build model function with show_plot_Y is equal to True, plot the actual values versus predicted values in the form of a line chart, and if you've called it with show_plot_scatter equal to True, display a scatter plot in matplotlib with the original X and Y values of the test data and the predicted line. Please note that this scatter plot will only work if you've used just a single feature for training. And finally, we'll return from this build model function the training score and test R square score for this particular model. All of these steps involved in building and training our model are ones that we've seen before, I've just wrapped them all up into a nice helper function. There is one more helper function that I need to define before we move on to the actual regression models. This is the compare_results function. This is the function that will quickly print out the training, as well as test scores for all of the regression models that we've built so far. This function uses a for loop to iterate through all of the keys in our result dictionary and then prints out the kind of regression that was performed, the training score, as well as the test score.

Single Feature, Kitchen Sink, and Parsimonious Linear Regression
Before we dive into other regression models, let's take our helper functions for a dry run by using them to perform simple linear regression. We're intimately familiar with the details of how linear regression works, and we are familiar with the linear regression estimator object as well. Let's see how we can set it up to use the build model helper function. This linear_reg function takes in training data, x_train, and target values, y_train. Within this function, we instantiate the LinearRegression estimator object with normalize is equal to True and call model.fit on this training data. Once the model has been trained, we return an instance of this fully-trained model to the caller of this function. This is the helper function that we'll pass in to build model, and here's how we'll do it. We invoke the build_model function that will train our regression model and calculate the training, as well as test scores and assign these results to the result dictionary object. We'll save the training and test score in the result dictionary with a meaningful key. So we have regressed to find the values of mpg, this is a single linear regression. Single linear, because we just use one feature for the regression, and let's take a look at build_model for this. The linear_reg function that we just defined is the first input argument, that is our regression function. The target value that we want to predict using this model is mpg, the input feature that we use to train the model is just one, that is the weight of the car, the original dataset is automobile_df, and we want to show Y values, actual versus predicted. Run Shift+Enter to build and train our linear regression model using our helper function, and here is the training and test scores for this model. We also have a nice little line chart here with predicted values in blue and actual values in orange. You can see how easy it is now to run different regression models using our helper function. Let's try this once again. This time we'll perform our kitchen sink linear regression with all of the features as input. The result of this regression will be present in the mpg - kitchen_sink_linear key, and the features we use in our training data are cylinders, displacement, horsepower, weight, and acceleration. I simply execute this code, the build_model function takes care of the rest. Our kitchen sink regression performed decently well this time around, training score of 70%, test score of around the same. But you don't really need to throw the kitchen sink at your linear regressor, you'll find that a more parsimonious regression with a few selected features performs just as well. Here is a parsimonious regression using the same linear regressor estimator object, we'll only use the horsepower and weight features in our training data. We've dropped the number of features down from five to two, but because these were the most significant features, we see that the training score and test scores for our regression are still high. Now, using five columns versus two columns when our dataset is small may not make much of a difference, but imagine if you had a dataset with millions of records. Going from five columns down to two might save you huge amounts of time during training. Now that we've performed linear regression using a single feature, the kitchen sink regression and finally a parsimonious regression, let's compare results and here are all of the training and testing scores for all of the regression models that we've just built and trained right here for you, set up side by side. This one screen allows us to quickly compare how the different models have done.

Lasso Regression
The lasso regression model uses L-1 regularization to add a penalty to our loss function. The objective of this penalty function is to reduce the magnitude of regression coefficients so that we don't end up with an overly complex model. Regularization is a technique by which we prevent our models from overfitting on the training data and build more robust solutions. We'll now see how we can perform lasso regression using scikit-learn libraries. Define a function called lasso_reg, which takes in the training data, as well as target values, and within this function instantiate and train a lasso estimator object. When you explore the documentation for each of these estimator objects for regression models in scikit-learn, you'll find that they have a number of different parameters that you can tweak to design your model exactly the way you want to. An important hyperparameter that you specify when you build your lasso regression model is alpha. Alpha is the constant that you use to multiply the L-1 regularization term. The default value for alpha is set to 1, and higher values of alpha imply more regularization. If you set alpha to 0, this completely eliminates the L-1 penalty term, which means Lasso regression defaults to ordinary linear regression, least squares regression. The way lasso regression is implemented behind the scenes in scikit-learn, for numerical reasons this estimator should not be used with alpha equal to 0, something for you to watch out for. In order to perform ordinary least squares regression, simply use the LinearRegression estimator object. Let's build and train a lasso regression model by calling the build_model function. This is a kitchen sink regression, as you can see, I've passed in all five features here. We've seen just a little bit earlier that kitchen sink models with linear regression don't really perform well, but if you take a look at the training and test R squares for lasso regression, you'll find something interesting. You'll find that the model performs better on the test data with a test score of almost 73%. Lasso regression models are regularized. The penalty that we've imposed, the L-1 penalty, force model coefficients to be smaller in magnitude. This results in a simpler and more robust model, which performs well on test data. So if you're performing kitchen sink regression because you don't know which features in your data are significant, it's better to use a regularized model. Let's quickly call the compare_results function here in order to see all of the training and test scores in one place. You can see that the kitchen sink linear regression didn't really perform as well as the kitchen sink lasso regression. The R square for test data was almost 73% for our regularized model, whereas it was just around 70% for our non-regularized linear regression model.

Ridge Regression
The ridge regression model is another one that imposes a penalty on an overly complex model by using regularization. As we've studied here, ridge regression works exactly like lasso regression, it reduces the magnitude of regression coefficients by adding L-2 regularization in the loss function. The L-2 regularization term is the L-2 Norm of the coefficients, which is the sum of the squares of the coefficients which we use to add as a penalty. Once again, the alpha parameter here is used to determine the strength of the regularization. This should be a positive floating point value, larger values imply stronger or greater regularization. Once again, because this is a regularized regression model, we'll perform kitchen sink ridge regression. We'll throw in all features here and see how kitchen sink regression performs using the ridge regularized model. And you can see here from the training R square and the test R square that this particular model didn't really perform well for this dataset. I'll now call compare_results for us to quickly take a look at how this model performed against other models that we had trained earlier. For this particular dataset, the lasso model regularized using L-1 Norm performed better than the ridge regression model. Remember that this in no way implies that in absolute terms one regression model is better than the other, it depends on your dataset, it depends on other model parameters, which we haven't really tweaked here. Both lasso and ridge are regularized models, which impose a penalty on more complex models or higher value of coefficients. The penalty that they impose whether it's the L-1 Norm or the L-2 Norm of coefficients is what is different.

Elastic Net Regression
We'd spoken earlier about the fact that the lasso and ridge regressions were essentially the same except that they imposed different penalties in their loss function. The lasso regression model uses the L-1 Norm of coefficients as its penalty function, the ridge regression model uses the L-2 Norm. And the elastic net regression model combines both lasso and ridge regression. The elastic net model that we're going to implement here in this demo reduces the magnitude of regression coefficients by applying both L-1, as well as L-2 regularization. In what combination you want to combine L-1, as well as L-2 regularization is up to you, it's a parameter you can tweak. Let's study some of the parameters that we have here that go into our elastic net model. The first is the alpha parameter that determines the strength of the regularization. Alpha is a constant that you use to multiply the penalty terms in your loss function; the default value for alpha is 1. The l1_ratio is what is called the elastic net mixing parameter. This is the ratio that you tweak in order to determine in what combination you want to apply L-1 regularization and L-2 regularization. If L-1 ratio is equal to 0, that is completely an L-2 penalty. An L-1 ratio of 0 implies ridge regression where you use the L-2 Norm of your coefficients as the penalty function. An l1_ratio of 1 swings to the other end, that is lasso regression where you use the L-1 Norm of your coefficients as the penalty function. We've chosen an l1_ratio of 0.5 here to get a mix of L-1, as well as L-2 regularization. We set normalize to False and run this model for a maximum number of 100, 000 iterations. When I worked on this particular dataset, I found that 100, 000 iterations gave me decent results. You can try this on your own with lower values of max_iter and you'll find that the results are not as good. When you're working with data in the real world with thousands, maybe millions of records, you should normalize your data, but for this toy dataset you'll find that even with normalized False, we do just fine. Let's study some of the other parameters here which we haven't encountered before, warm_start is equal to True. If you want your model to have a warm start, that is if you want your model to reuse the solution of the previous call made to fit this model as the initialization for the next step, you'll set warm_start to True. If you want to erase all previous solutions and start afresh, you'll set warm_start to False. You'll find that the number of different models in scikit-learn's library accept the warm_start parameter. The equivalent_to parameter is something that I'm going to pass in in order to understand what this elastic net model is equivalent to. This is something that we explicitly pass in when we invoke this function. We first print out what this model is equivalent to, then instantiate the ElasticNet regressor with the parameters that we passed in and call fit on the model to start training. Now in order to tweak the value of the parameters that I pass into my regression function, I'm going to use this partial function available in Python. This allows me to partially specify the parameters for a particular function. Now I'm going to build and train an elastic net model and I'm going to set the model parameter such that it performs ordinary least squares regression. Observe a partial specification of the parameters to the elastic net regression function. When you set alpha to 0, it means that no penalty will be imposed while training our model. This is equivalent to simple linear ordinary least squares regression. Now, the way elastic net is implemented in scikit-learn's library, this will trigger a convergence warning when you try to implement OLS using the ElasticNet estimator. This is not really recommended. If you want to perform OLS, use the LinearRegression estimator object. Let's go ahead and run this anyway; this is a kitchen sink elastic net regression, the equivalent of OLS. We get a training score and a test score, but the convergence warning is hidden here because we're ignoring warnings. That's the specification we gave up front before we started this notebook. Let's run elastic net regression once again, but this time we specify l1_ratio equal to 1; this is the equivalent of lasso regression. The multiplier for our penalty terms alpha is set to 1. This is a regularized model, the equivalent of Lasso regression, I'm going to pass in all of my features for kitchen sink regression. And just like our lasso regression model did well earlier, the elastic net model with l1_ratio equal to 1 also does well. The R square score on the test data is 72%. Let's run elastic net regression once again, this time we set the l1_ratio to be equal to 0, so elastic net performs the equivalent of ridge regression with just L-2 regularization. Once again, this is a kitchen sink regression with all of our features, and you can see that ridge regression also performs reasonably well on our dataset. But if you're using elastic net as your regression model, what you really want is the ability to control how much L-1 regularization and how much L-2 regularization should be applied to your model. Here I specify a ratio of 0.5 for both kinds of regularization, and here is what my elastic net scores look like. Both training and test scores are high, test scores are higher than the training score, indicating that this is a fairly robust model. If you've forgotten what the results of the previous runs were, you can always call compare_results in order to view results across all regression models.

Module Summary
And this demo brings us to the end of this module on building regularized regression models. We spoke briefly initially about choosing regression to solve problems and saw a quick overview of the various estimators that scikit-learn offers for regression analysis. We spoke about the drawbacks of overfitting on the training data. Mean square error regression, which is what we have been working with so far, may have this problem where you overfit a model such that it performs well in training, but poorly in the real world. Mitigating overfitting is an important aspect of building ML models. We spoke about regularization to mitigate overfitting and regression models that use this technique to make better, more robust models. We then discussed three regularized regression models that you can build and train using scikit-learn, lasso, ridge, and the elastic net regression models. We saw that lasso and ridge regression differ only in the penalty term that they apply to the objective function, and elastic net is simply a combination of lasso and ridge. In the next module, we'll move beyond regularized regression models to other techniques that you can use for regression, such as support vector machines, nearest neighbors regression, stochastic gradient descent regression, decision trees, and so on.

Performing Regression Using Multiple Techniques
Module Overview
Hi, and welcome to this module on Performing Regression Using Multiple Techniques. We'll first start off by seeing how we can choose the right regression algorithm based on the size of your dataset and the number of features, or X variables. We'll discuss the concepts and principles behind support vector machines, which are commonly used for classification algorithms. We'll see how the same principles extend to support vector machines for regression. We'll then move on and get a conceptual understanding of the nearest neighbors algorithm to perform regression analysis. We'll discuss the two flavors of this algorithm, k-nearest neighbors and radius neighbors. If you're working on a large dataset with many features, stochastic gradient descent for regression works very well. This is an iterative algorithm, which iteratively includes additional data points from your dataset in order to find the best regression coefficients. We'll then move on to another classic machine learning technique, the decision tree, and you'll see how decision trees can be used for regression analysis. And finally, we'll discuss an algorithm that works very well when you have a small size of datasets, but with many features, least-angle regression, or LARS.

Choosing Regression Algorithms
In the previous model, we got a brief overview of the different regression estimators available in scikit-learn. Here, we'll talk about choosing regression algorithms based on your data and number of features. Here is a matrix representation of a number of features along rows and size of the dataset along columns. So we've divided our dataset sizes into three broad categories, small, medium, and large. And the number of features can be few, moderate or many. As you can see, these are fairly coarse distinctions, but these are useful in figuring out which estimator is the right one to use. Now, if you have more than 100, 000 data points, you choose to go with the stochastic gradient descent. The SGD algorithm works very well on very large datasets which have a large number of features. If the size of the data that you have to train your model is small, but you have many features, that is there are more features than samples in your dataset, you'll go with the least-angle regression, or LARS. For medium-sized datasets with a moderate number of features, you may have many features initially, but only a few of those features are useful, the lasso and elastic net regression models are what you might find the best. The lasso model tends to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features. The lasso model prefers those features which it considers more significant and tries to eliminate the others. If you have a medium-sized dataset and most of the features in your data are useful, that's when you choose to go with the ridge regression model. If the dataset that you're working with are fairly small and you have a moderate number of features, support vector regression with a linear kernel might help. This also works for medium-sized data where there is non-linearity in the dataset itself. If you have very small data with nonlinearity, you might choose to use support vector regression with the RBF kernel. If you have just a few features and a medium-sized dataset, decision trees and ensemble methods work well. Ensemble methods are when you have many big models which come together to build a more robust model. And finally, the size of your dataset is large and you have just a few features, ordinary least squares regression, that is the very basic regression model might work best. These are all the options available to you to build regression models in scikit-learn. This little matrix here shows us a quick look up of when you would choose to use which regression model. This can be the starting point for choosing your regression algorithm.

Support Vector Regression
In this clip, we'll discuss the intuition behind support vector regression algorithms. Now, if you've heard of support vector machines, or SVMs, you know that they're typically used for classification problems. Support vector regressors use the same underlying principles as support vector machines with a different objective function. The intuition is the same, the objective function is different. Let's discuss the simplest possible data in one dimension, which can be represented using a line such as a number line. Such unidimensional data can also be separated or classified into categories using a single point. A single point can be used to separate this data. Few other words in the review, maybe those are negative reviews. There are large number of words in a review, maybe they are positive. Here our separation point is number of words equal to 10. This is just an example. Let's extend this to add one more dimension data in two dimensions is often more useful than data in a single dimension. Bidimensional data points can be represented using a coordinate plane. We have two bits of information about reviews represented on the X and Y axis, time when the review was posted and the number of words in the review. Bidimensional data points can be represented using a plane and can be classified using a line. You just draw a line through your data. On one side of the line, we have positive reviews and on the other side, we have negative reviews. And you can extend this idea to data in n-dimensions where n is equal to the number of features in your data. For the sake of simplicity, we've just looked at three dimensions here in this visualization. N-dimensional data can be represented using an N-dimensional hypercube and classified using a hyperplane. And this is exactly what support vector machines do. Support vector machine classifiers find that hyperplane that best separates points in a hypercube. And we use that hyperplane to classify our data. Now, in the ideal situation our data is linearly separable. We have one decision boundary. Let's go back to the two-dimensional view for simplicity. And this is a hard decision boundary that can separate all of our data points. Now, it's pretty obvious that some data points will be close to this boundary and others will be further away. The nearest instances on either side of the boundary are referred to as support vectors, and that's where this classifier or regressor gets its name from, support vector machines. These are the data points that support this decision boundary for categorization. Support vector machines try to find the widest street between the nearest points on either side. This is support vector machines as applied on classification problems. The job of the classifier is to find this widest boundary between the nearest points on either side and use that to classify instances. The principles underlying support vector machines remain the same whether you apply to classification problems or regression. That's why we spoke of classification first. Let's understand how support vector machines for classification differ from support vector machines for regression. When we use SVMs for classification, the machine learning model will try to find the widest margin with the most distance from the nearest points. That is from the support vectors. The classification is better if this margin is as wide as possible. Support vector machine regression, on the other hand, tries to find the line that best fits the points. And this is a regression line that lies between the support vector machine margin. Now the whole idea of SVM classification is that you want no points to be within the margin. That's what divides your data cleanly into categories. And this is where SVM regression is different. It has a different objective function. It seeks to maximize the number of points inside a margin. You want all of your points to lie inside the margin and then for the regression line. So when you're performing SVM classification and you find this margin points far away from the margin are good, that means they improve the objective function value of the classifier. On the other hand, with SVM regression points which lie far away from the margin are considered bad. They worsen the objective function value. Ideally, you want all of your points to lie within the margin. With SVM classification outliers which are on the wrong side of this line are penalized. So if you have a red point on the side of the blue points, that's an outlier and that is penalized. With SVM regression, it doesn't matter what category your points belong to, whether it's red or blue, but points which are just far away from the margin are penalized. With SVM classification, when you run your classifier model, the width of the margin is found by the optimizer. The ML model tries to make this margin as wide as possible. With regression, the width of the margin is something that you specify upfront, and this requires another hyperparameter called epsilon. The epsilon value that you specify is a part of the model design in regression.

Implementing Support Vector Regression
Another regression model supported by the scikit-learn library is support vector regression. There are two estimator objects that you can use in scikit-learn to perform support vector regression, SVR or LinearSVR. They're essentially the same, the LinearSVR is simply the SVR with a linear kernel. A kernel is simply a shortcut function that the algorithm uses behind the scenes in order to transform higher dimensional data into dimensions that are easier to work with. As per scikit-learn's documentation, the LinearSVR function offers more flexibility in the choice of penalties that you can impose and loss functions and skills better to larger datasets. Our dataset is fairly small here, so I'll go with the SVR estimator object. The support vector regressor tries to fit as many points as possible into a margin surrounding the best fit line that it calculates. If this margin were larger, more points would be able to fit in, but maybe this best fit line would not be a good approximation of the underlying data. So there is a tradeoff here. The parameter that we specified here, epsilon, is the margin or the epsilon tube to use with our model. The margin into which the support vector regressor tries to fit as many points as possible is given by two multiplied by this epsilon. C is a penalty that we apply to points which lie outside the epsilon tube while calculating errors for our best fit line. When points lie outside the epsilon tube, that's called a margin violation, and this penalty seeks to reduce the number of margin violations when we fit our model. If you specify a very high value for C, that imposes a heavy penalty on outliers. Let's now use the support vector regressor to build entry in our machine learning model on our automobile dataset. Once again, this is a kitchen sink regression. Given all of the constraints that we've imposed on this regressor, this model performs decently well. Its R square on the test data is about 71.5 %. If you want to compare the results of this model with other models that we've trained in the past, simply call compare_results. Here is the result of our kitchen sink SVR regression model. You can compare it with elastic net, that's what we performed last. You can see that for this particular dataset and based on the parameters that we've chosen, the two models are fairly close.

Nearest Neighbors Regression
This clip will study another technique for regression analysis, nearest neighbors regression. Nearest neighbors regression uses the training data to find what is more similar to the current sample. So you have the training data which will find the locations of all of your samples in an N-dimensional hypercube, and once you have a new sample come in, it'll try and find what set of points are closest to this new sample. Let's understand this visually using an example. Here are all of the data points that make up your training data. So your training dataset makes up your model, it has a rocket, building, signal, pig, shop, these are your data points, and every element in the training data has an associated y value. And this y value is what you're trying to train your model to predict. Let's say you have a new sample come in. Predictions for a new sample involves figuring out which element in the training data this sample is similar to, the nearest neighbor. A neighbor is any data point that is similar to this data point, and we'll talk about what similarity means in just a little bit. So you have your new instance come in, it'll check to see whether it's similar to the rocket, to the buildings, to the signal, to the pig, to the shop. We'll find that the new training data is the most similar to a shop, you'll predict that it's a shop. Now, rocket, buildings, signal, pig are of course just categories that we are working with, but with regression, these are numeric values that you're trying to predict. When you're working with machine learning models, the input that you feed in comprise of numeric values, and you can imagine that every data point is a point on an N-dimension hypercube. So how do we calculate the neighbors of a sample? Well, there are a variety of distance measures that you can use to calculate neighbors. And there are a variety of distance measures you can use to calculate the distance between points, the most common is, of course, Euclidean distance. There is also the Hamming distance, the Manhattan distance. There are also additional distance measures such as the cosine distance. Once you have the coordinates of two data points, each of these distance measures simply apply a different formula to calculate the distance between those two data points. The Euclidean distance is by far the most common and one that you might be familiar with. It is the distance between two data points as the crow flies. The shortest possible distance. There are two techniques that you can use to perform nearest neighbors regression. By far the most common and popular one is the k-nearest neighbors regression algorithm. Another variant is the radius neighbors regression algorithm. K-nearest neighbors regression involves finding the K closest points to your data, and you'll find the average value of the K nearest neighbors. This average of the y value of the k nearest neighbors is your predicted value for this new sample. With radius neighbors regression, you'll define your neighbors as all of the data points that lie within a certain radius, and you'll find the average y values of neighbors within that radius. Let's quickly visualize this on our 2D plane. K-nearest neighbors, let's say you have data points scattered as you see here on screen, and you have a new data point for which you want to predict values. You'll find that k-nearest neighbors to this data point and then make your prediction. You'll average the y values of these k-nearest neighbors, remember, K is something that you can define for a model, and then make that y value your prediction. In the case of radius neighbors regression, you'll draw a circle with a certain radius around the new data point for which you want to predict the y value. All of the data points which lie within that radius are neighbors, and then you'll find the average y value of all of the neighbors. The way you'll determine neighbors is what is different here.

Implementing K-nearest-neighbors Regression
The scikit-learn library also supports regression based on k-nearest neighbors. When you build and train your k-nearest neighbors regression model, it calculates a similarity measure across all of the data points present in your dataset. Then, when your model encounters a particular test instance it hasn't seen before, the KNN regressor finds the k-nearest neighbors to that particular test instance and combines their values together in some way in order to make a prediction for your test instance. Now, the number of neighbors you want your regression model to use in order to make predictions is something that you can specify. The default value for the KNN regressor model is 5 neighbors; we've specified a value of 10 here. The right value of number of neighbors depends on your dataset, and it's something that you find using something called hyperparameter tuning that we'll see in the next module. Behind the scenes, this regression model can use a number of different algorithms to compute the nearest neighbors. It can also use brute force. By default, this estimator object chooses the best algorithm based on your data. Let's build and train a KNN regression model and perform kitchen sink regression. And if you take a look at the training and test R square scores, you'll find that this model performs the best on our dataset. Both scores are high at around 75%. If you want to see how this model performs against others that you've trained before, use compare_results. The training and testing scores of all models are available here, and you can see at a glance that the k-nearest neighbors regressor has performed the best so far on our dataset.

Stochastic Gradient Descent Regression
SGD stands for stochastic gradient descent. This is an iterative model where you use multiple iterations to find the best linear model that fits your underlying data. Now we know that the best fit line here is the regression line, and it is that line which has the minimum least square error. Now we have to find the regression coefficients that give us this best fit line. Assume that these regression coefficients are w and b and are expressed along the two axes that you see. The mean square error is expressed along the third axis, the Z axis. Now, for different values of w and b, the mean square error, or the loss function, will have a different value, and imagine that this hypothetical curve represents the different values of MSE for different regression coefficients. Now, somewhere along this curve is the smallest possible value of the mean square error, and what we are trying to do when we build our regression model is to find those regression coefficients, w and b, which give us this mean square error that is as small as possible. So what are the regression coefficients that will give us the smallest value of MSE? Well, that's what stochastic gradient descent tries to figure out iteratively. The way this really works is that it will start off at some initial value of the mean square error. So w and b, or your regression coefficients, have some initial value. It'll then perform gradient descent down this slope in order to figure out the smallest value of MSE and the values of w and b that correspond to the smallest MSE. So the stochastic gradient descent optimization algorithm will iteratively tweak the values of your regression coefficient in order to walk down the slope to converge on the best value of MSE. And this is what the training process of the stochastic gradient descent regressor involves, finding these best values, the regression coefficients of your model. The gradient descent term comes from the fact that your model has to walk down the slope to converge on the best values of MSE in order to give you your regression coefficients. Thus, by considering your data points, a single data point at the time or in batches, stochastic gradient descent iteratively converges to the best possible model. Here are a few highlights of the SGD regressor. You can use different loss functions to train your SGD regressor models. If you use the mean square error loss, that will give you the ordinary least square regression model. You can also use different loss functions to implement the lasso, ridge, and elastic net regression models. Remember that these three are regularized regression models where we add a penalty to the loss function. That's the only difference. SGD training also works very well for very large datasets.

Implementing Stochastic Gradient Descent Regression
In order to perform regression using stochastic gradient descent, we are going to scale and standardize our dataset. This is a preprocessing function that will apply to our data before we pass it into our regression model. You've already seen an example of how we can standardize our dataset to have mean equal to 0 and variance equal to 1 using the preprocessing.scale function. The scikit-learn library also offers a standard scaler estimator object, which you can use to standardize your data. I've wrapped up this object here in the apply_standard_scaler of function. In this demo here, we'll see how we can use the scikit-learn library to perform SGD regression where SGD stands of stochastic gradient descent. Mean square errors regression can be performed using analytical techniques where you fit a formula. The stochastic gradient descent regression performs numerical optimization, it uses one training instance at a time to find the best model parameters for your regressor. An important parameter that you need to pass in to your SGD regressor is the number of iterations or epochs for which training should run. For each epoch, it'll consider just one instance of your training data, and we will do run for about 10, 000 epochs. The tol parameter here stands for tolerance, and this is stopping criterion for our model. If you set a stopping parameter, which is typically a floating-point value, the training iterations of your model will stop when the loss at each consecutive iteration falls to below this tolerance. When loss reduces less than this tolerance, what you're seeing is that your model isn't really improving that much, so stop training right there. Let's run a kitchen sink regression here using our stochastic gradient descent regressor, and let's look at the training and test scores. Our model performed reasonably well during training, but the test scores were not that great. You can try changing the number of iterations for which you'll train your model to see if this improves.

Decision Tree Regression
Another technique that you can use to build a regression model is decision trees. Now decision trees, once again, are typically used for classification, so let's discuss the underlying principles behind the decision tree for classification before we move on to decision trees for regression. You're trying to categorize a sports person either as a jockey or a basketball player. You know that jockeys tend to be light to meet horse carrying limits, but on the other hand, basketball players tend to be tall, strong, and heavy. Those are the needs of the game. So if you're just looking at the sports person and you want to determine whether he or she is a jockey or a basketball player, you'll use your intuition. You'll say jockeys tend to be light and not very tall so that the horses can run faster, while basketball players tend to be tall and they also tend to be quite heavy. Observe here that you're making decisions based on the height, as well as the weight of the sports person, and this is what decision trees do. When you feed in a bunch of training data to your decision tree algorithm, it will set up a tree structure based on this training data, and this tree structure will help make decisions based on rules that it has gleaned from the training data. Based on the training data, you might get a decision tree model that looks like this. The first decision might be based on the weight of the sports person. Is it greater than 150 pounds or below 150 pounds? If it's greater than 150 pounds, you might categorize that individual as a basketball player. If the sports person happens to be less than or equal to 150 pounds, you might take into account the next variable, the height of the individual. Is the person taller than 6 feet, or is he or she shorter than 6 feet? Greater than 6 feet maybe means that he is a basketball player, under 6 feet might mean he is a jockey. Here we've used two variables and two decisions, a decision based on the weight of the individual and another decision based on the height of the individual. Your training data of course might have many features, and every feature may not involve such a simple decision. But that is what your decision tree algorithm is trained to extract. You feed in data to your decision tree algorithm, the training data, and it'll use the knowledge that it has gleaned from this training data and fit knowledge into rules. And each of these rules will involve a threshold. This is the threshold that you'll use to make decisions. And this is the entire concept behind a decision tree model. Remember that the order of the decision variables will make a difference, and these rules and the order of the decision variables are found using the decision tree algorithm. Such a decision tree built on your training data is often referred to as CART. CART stands for classification and regression tree. CART, because these decision trees can be used for both classification, as well as regression. Let's see an example of using decision trees for classification. You feed in the weight and height of sports persons and you get to know whether he or she is a jockey or a basketball player. This is the example that we've worked with so far. Once the decision tree has been built for any new individual, you'll traverse the tree to find the right node based on his height and weight. You'll then return the most frequent label of all of the training data points at that node. Most of the players on this node are basketball players. That is a basketball node. The same principles can be extended to use decision trees for regression models, you feed in the weight and height of individuals, and then you predict the number of years that that individual spends in his or her career. Remember, regression involves the prediction of a continuous numeric value such as number of years in career. Once again, you will traverse the nodes of your decision tree to find the right node, and once you're on the right node, you'll return the average number of years that sports people in that node have spent in their career. And that is your prediction. We use the same decision tree structure, but how you perform your prediction is what is different when you use decision trees for regression.

Implementing Decision Tree Regression
The scikit-learn library supports building regression models using decision trees as well. You can use the classification and regression tree algorithm to fit a decision tree onto your training data. A decision tree splits your underlying data into subsets where every subset contains points that it considers similar. The data is repeatedly split into subsets to form a tree structure, and the shape of the tree depends on the constraints that you specify. Once your decision tree regression model has been built given a test instance, you will traverse the decision tree based on that test instance and return the average value of all training instances in the corresponding leaf node for this particular test instance. Here we've instantiated a DecisionTreeRegressor estimator object with a max depth of 2. Decision tree models use several parameters to control the size of the tree, otherwise the trees can get arbitrarily large and consume a lot of memory. Max depth is one such parameter available in the decision tree regressor. We can now build and train a decision tree regression model exactly like how we did with previous models. This is once again kitchen sink regression. You can see that the training score for this model, about 73%, the test R square is a little lower at 66%.

Least Angle Regression
Least angle regression is a regression technique that works well when you have many more features than samples in your dataset. You have a small dataset, but with many features for each record. Least angle regression is a technique that relies on selecting x variables that have the highest correlation with the unexplained y variable. The variable with the highest correlation is the one which has the least angle, and that's what gives least angle regression its name. Let's understand this least angle idea before we move on to the actual regression technique. Here we have two vectors, X and Y, that are parallel to one another, and the angle between them is 0 degrees. You can see that these two vectors are perfectly aligned and you can imagine that these two vectors are highly correlated. They have a correlation of 1, which is the highest possible value. A small angle of 0 between vectors corresponds to a high correlation. Let's look at opposite vectors here. We have vectors X and Y, which are opposite to one another. They're pointing in opposite directions. The angle between them is 180 degrees. Such vectors are said to be perfectly opposed and they have a correlation of -1, which is the lowest possible. Let's consider these two vectors once again, X and Y, this time they're orthogonal. X and Y are at 90 degrees. Orthogonal vectors represent data that are completely uncorrelated. X and Y are unrelated, they are independent variables. Using X to predict Y is not a good idea. So when you choose X variables to predict Y, you want to pick those X variables which have the least angle, that is a higher correlation. This is the fundamental principle behind least angle regression. And let's see this in the form of some pseudocode. You're performing LARS regression and you have a bunch of X variables. Initially you'll set all of these X variable coefficients represented by data equal to 0. You'll perform correlation analysis or find that X variable which has the least angle with the Y that you want to predict. Find the predictor Xj most correlated with y. And then you'll steadily increase the coefficient of Xj, this coefficient is beta j. As you keep increasing this beta of the j, you'll find that there is some other x variable represented by X of i that is more highly correlated than X of j with y. At this point, this new variable, Xi, will be added to your model as well, and you'll increase the coefficients beta i and beta j corresponding to Xi and Xj. And you'll continue this stepwise process until all of your x variables are present in your regression model. This is the fundamental principle behind LARS regression, and there are several advantages to using this regression analysis technique. It works well when the number of dimensions or features are far larger than the number of points in your dataset. It's an intuitive and stable algorithm which gives good results in many situations. It's the equivalent to forward stepwise regression where you add variables in one by one. The one drawback of using LARS is that it has problems dealing with highly correlated x variables. Multicollinearity is a major issue when you're using LARS regression, so make sure you explore your x values. Try and remove those x features that are highly correlated before you use LARS.

Implementing Least Angle Regression
Here is the LARS regression model that we'll study in this particular module, and that is LARS regression. LARS stands for least angle regression and is an algorithm typically used for high dimensionality data. When you use LARS regression, it calculates how correlated each of your features are with the target. Features are included in the regression model in the order of their correlation with the target with the highest correlated features added first. There is another kind of regression called stepwise regression which does this as well, however, LARS is much more efficient than stepwise regression. LARS is the same complexity as ordinary least squares regression. A parameter to this estimator object is a number of non-0 coefficients in your regression. That's what we'll set to 4 here. Use the build model_function to build and train this LARS regression model. Once again, this is kitchen sink regression. Let's take a look at the training and test scores. This model performs okay, decently well. A training R square of 72% and a test R square of 62%.

Regression with Polynomial Relationships
What if the relationships that exist in your data cannot be captured using a liner model? So far, we've assumed that the relationship between y and x is linear. That is x a polynomial of degree 1 in our formula. This is a basic relationship that we're trying to capture when we build a linear model on our data. Y is equal to Wx + b. X is raised to the power 1. But what if this linear relationship that we've assumed is not really right for our data? What if the relationship between y and x in our data is a polynomial of degree 2? Maybe it's of this form that you see here on screen. Vx2 + Wx + b. This is a polynomial of degree 2. You can extend this to other polynomial relationships as well of degree 3, 4, and so on. Now in such situations, when the relationship between y and your x variables is non-linear, you'll find that the linear model performs poorly. A straight line is not the best representation of your data. In such situations, a quadratic fit might be a better model, it might perform better for predictive analytics. So your data looks this way, but you still want to use a linear model on your data. Well, there is a way out. You'll simply change or transform your data to generate polynomials of a certain degree of all input features. So if your input features are just x, you'll add additional polynomials. You'll generate x2, x3, x raised to the power of 4, all of the polynomials that you'll need, and then on this polynomial data, you can fit a simpler model such as a linear model. A linear model is simple and robust and you might have many reasons for using it, which is why if you have a nonlinear relationship, apply transformations or generate polynomial features of your underlying data and then fit the simpler model.

Module Summary
And with this, we've come to the very end of this module where we took a look at a number of different techniques to perform regression analysis. We first started off by understanding how we can choose our regression algorithms based on the dataset size and the number of features or x variables in our data. We then got a conceptual understanding of each of these regression techniques in some detail, starting off with support vector machines, which are typically used for classification problems. We saw how SVMs for classification and regression are similar, yet different. The main difference is on the objective function that we try and minimize. We then moved on to nearest neighbors regression and discussed two flavors, k-nearest neighbors and radius neighbors. We then discussed stochastic gradient descent, or SGD, for regression analysis, which is an iterative technique that walks down the gradient slope in order to find the best regression coefficients. Decision trees are another machine learning technique commonly associated with classification. We saw how the same principles of decision trees could be used for regression analysis. And finally, we discussed least angle regression, or LARS, a regression technique that selects x variables that have the highest correlation with the unexplained y variable. In the next module, we'll discus the best design for our regression model, which we can find using hyperparameter tuning.

Hyperparameter Tuning for Regression Models
Module Overview
Hi, and welcome to this module on Hyperparameter Tuning for Regression Models. We'll talk about hyperparameters and what they mean, and we'll learn to distinguish between model inputs, model parameters, and model hyperparameters before we move on to hyperparameter tuning support in scikit-learn. We'll see that the scikit-learn library offers something called grid search for hyperparameter tuning. Grid search works very well, but can be computationally intensive as you have to build and train multiple different models. We'll talk about an alternative available to grid search, that is random search of the hyperparameter space. We'll then perform hyperparameter tuning on different regression models, including lasso regression. We'll see that lasso regression has just the one hyperparameter to tune, alpha, the penalty coefficient. Other regression models such as decision trees give you many more configuration properties that you can tune, such as the depth of the decision tree, the number of samples in each node, and so on.

Hyperparameter Tuning
Before we can use hyperparameters to define our model, you need to understand what hyperparameters are. These are model configuration properties that define a model and they remain constant during the training of the model. An easier way to think of hyperparameters is that they are part of the model design. It's something that you specify that makes up the design of your model. When you talk about a machine learning model, there are typically three bits of data that you associate with the model, model inputs, which is what you use to train the model; model parameters, these are what you're trying to figure out during model training; and finally, model hyperparameters, which make up the design of your model. Model inputs refer to the training data, this is the data that your model uses to learn. You feed in training data during the training process of your model, and this is what the model uses to find the model parameters. The model parameters here are the regression coefficients in a regression model. Based on whether you have a regression model or a classification model, model parameters will of course be different. In the case of regression, the coefficient and the intercept are your model parameters. And finally, you have model hyperparameters. These are the configuration properties for your model. If it's a decision tree model, the depth of the decision tree could be a hyperparameter. If it's a regularized regression model that you are building, the value of alpha that you use to multiply the penalty function is a hyperparameter. The best way to distinguish model parameters and hyperparameters is that model parameters learn or change during the training process, model hyperparameters remain constant. That's something you specify up front. Hyperparameter tuning in scikit-learn is performed using grid search. Here is where you specify all possible values for a hyperparameter, all possible values for alpha, say. All of these values form a grid where every cell is a candidate model, a model that you want to test. So every cell is a particular design of a model that you want to evaluate. You will then use GridSearchCV, where CV stands for cross-validation, to evaluate each candidate model. You don't need to worry about what exactly cross-validation is, it's just a technique that you use to evaluate models, a validation technique. Once you use GridSearchCV, scikit-learn is responsible for ensuring proper evaluation and cross-validation. Grid search is very computationally expensive. If you have two hyperparameters and three values each, then you have 3 multiplied by 3, which gives you 9, so a total of 9 candidate models that you have to train and evaluate to find the best possible one. That can get pretty intense. So you need to be aware upfront that the cost and complexity of grid search can grow very, very quickly. And if you're performing grid search to find the best candidate model on a cloud platform such as AWS, GCP or Azure, cloud-based evaluation can quickly become very expensive. Also, grid search does not differentiate between important and trivial hyperparameters. You might know up front that these hyperparameters are important and these are not, grid search does not have this information. An alternative to grid search for hyperparameter tuning is random search of the hyperparameter space. Specify the important parameters and randomly pick values to find the best possible candidate model.

Hyperparameter Tuning for Lasso Regression Using Grid Search
So which machine learning model works best on your dataset? Well, you'll never know until you evaluate a bunch of different models with different parameters, and this is where we'll use hyperparameter tuning. In this demo, we'll perform hyperparameter tuning using the grid search object in scikit-learn to find the best model for our dataset. We'll write code for this demo in this Jupyter Notebook called HyperparameterTuningWithGridSearch. Let's set up the usual imports that we need to build and train our ML models, pandas, NumPy, Matplotlib, train_test_split, the r2_score, and the StandardScaler. We'll perform hyperparameter tuning for four different regression models, lasso, the k-neighbors regressor, the decision tree regressor, and the support vector regressor. We'll find the best model for our dataset using the GridSearchCV library in scikit-learn. Grid search is so called because it sets up the range of parameters that you specify in the form of a grid and trains a model with each combination of parameters. Once again, we'll work with the auto-mpg-processed dataset. You're familiar with this now. Read in the CSV file from your local machine and store it in a data frame. Hopefully you haven't forgotten what the data looks like. Here is a quick refresher if you need it. We'll use the different features of our car to predict the miles per gallon, the mpg, for that particular car. The training data that we'll use to perform hyperparameter tuning will contain all features in the underlying model, except for age. Set up your x variables by dropping the mpg and age columns from the original dataset. The target value for our regression is the mpg column. As usual, we'll use the train_test_split to split up our data into training and test sets. We'll use the test set to evaluate the best model that we'll discover using grid search. We'll first perform grid search to find the best parameters for our lasso regression model. Now, every regression model will have different hyperparameters that you can tune, and each model can have more than one. Here in this example we use grid search to tune just one hyperparameter, the value of alpha. Alpha is what we use to multiply the penalty terms of the lasso regression model. The alpha values that we want to test range from 0.2 to 1.0. Instantiate the GridSearchCV object with the lasso estimator. CV, as you know, stands for cross-validation. Specify the parameter dictionary for the different parameters that you want to use for alpha. Grid search will instantiate and train a lasso regression model for each value of alpha, so we have about seven values of alpha here, seven lasso regression models will be built and trained, each with a different value for alpha. The grid search object will then try and find the best model for your dataset by evaluating the model using three-fold cross validation. Your original training dataset will be split into three parts. Each model will be trained using three different runs. Two parts of the dataset will be used for training, and the third part will be used to evaluate your model. This is what cross-validation is. Your model will be scored and evaluated based on the default scoring mechanism used for that particular estimator object. In the case of regression models, the default scoring is the R square score. If you want to use grid search in more advanced ways, you can specify other scoring mechanisms to evaluate your models as well. Grid_search.fit will start a process of training on all of the different models and builds, and grid_search.best_params will return the parameters of the best model. And in the case of lasso regression here, you can see that the best value of alpha, the one which gave us the best model for our data set, is 1. But how did a value of alpha equal to 1 stack up against other models that grid search built and trained? We can get this using a simple for loop. Use a for loop to iterate through the number of models that grid search built. In this case it's 7, the number of values we had specified for alpha. For each of the models that grid search built and trained, the grid_search.cv_results_ variable holds the model parameters, the mean test score, and the rank of that particular model, whether it was the best model, the second best, and so on. Let's use this for loop to print all of this to screen. And you can see here that alpha is equal to 1 produced the best model, all of the other models are listed here as well with their corresponding ranks. You can see that the worst model at rank 7 was with alpha equal to 0.2, its R square was .68. And here is the model at rank 1 with alpha equal to 1 with an R-square of .69, not that much difference here, but still enough for grid search to say that this was the best possible model. Once you have the best possible value for the alpha parameter, you can instantiate a lasso regression model using this value of alpha and train it on your dataset. Let's print out the training score and the test score, and you can see that these are very close to what we got when we ran grid search. Thus, grid search is an extremely easy and convenient mechanism available in scikit-learn to perform hyperparameter tuning for your models.

Tuning Different Regression Models Using Grid Search
Now that we know how to use grid search to find the best hyperparameters for our model, let's train a few more regression models with different hyperparameters. Here we'll build a number of different KNN regressor models with different values for number of neighbors, ranging from 10 all the way through to 50 points. This is the parameter grid that we'd use to tune just one hyperparameter. Instantiate a GridSearchCV object with the KNeighborsRegressor estimator, pass in the parameter grid. We'll use cross validation equal to 3, and return the training score in order to compare the different models. Call fit to start the process of training all of these different models, all nine of them, and print out the best params. Given that we are building and training so many different models, this might take a while to run. On my machine it took about 15, 20 seconds. And the model which had the best R square score on this dataset was the one with 25 neighbors. Let's do a quick comparison using our trusty for loop across all of the different models that were built and trained by grid search here. We'll print out the params, the mean score, and also the rank of each model. Here are the R square scores, the number of neighbors, and ranks for all nine models. Of this lot, at rank 9 was the model with 10 neighbors. If you scroll down here, you can see that at rank 1 is our best regression model with 25 neighbors. When you add more neighbors, when you go up to 30 or 35, or even 50, the model becomes slightly worse. If you compare the model, which uses 20 neighbors, with that which uses 25, you'll see that they're really close, so there's not much to choose between the two. Grid search is what you'll use to figure out what the best model is. Once you've found the best value for the n_neighbors parameter, you can use it to instantiate a regressor, train the model on our training data, and use it for prediction. Let's print out the training and test scores, and you can see that these scores are very similar to what we got when we ran grid search. The scores will be a little different because we use the entire data to train the model here, whereas in the case of grid search, because we used three-fold cross validation, we used two-thirds of the training data for actually training the model and one-third to evaluate the model. Let's use grid search once again, this time to find the best possible decision tree regression model. The hyperparameter that we'll tune is the max depth of the tree. The maximum depth of the different trees built by our different models will range from 1 all the way through 8. The rest of the setup for grid search on the decision tree regressor is exactly the same. The one thing you have to ensure is that you pass in the right estimator object that you want to tune. Hyperparameter tuning tells us that the model for our dataset that performs the best is the one with max depth 4. I'll leave it to you as an exercise now to build and train an ML model using the decision tree regressor with a max depth of 4. Now there are multiple hyperparameters that you can use to design your model, and grid search can be used to train with multiple hyperparameters as well. In this example here, we'll train a support vector regression model using two hyperparameters, epsilon and C. Support vector regression works by trying to fit as many points as possible into a margin surrounding the best fit line, and epsilon determines the size of that margin. The parameter C, on the other hand, is the penalty that we apply to outlier points that lie outside of this margin. Grid search will train models for every combination of these hyperparameters. It will train a total of 8 models, 2 multiplied by 4. As we have many more complex models to train, this grid search takes about a minute or two on my machine. And the best model here was the combination where C was equal to 0.3 and epsilon was 0.1. Now that we know this combination, let's instantiate a support vector regressor using this value of C and this value of epsilon and train it on our dataset. And let's calculate the training and test score for this model. This model did fairly well. Training score of 68% and a test score of 65%. This is an example of how you'd use grid search to train multiple hyperparameters to find the best possible model. The more hyperparameters you add, the longer it takes for grid search to find the best model.

Summary and Further Study
And with this demo, we come to the very end of this module on hyperparameter tuning for regression models. Before we got to the hands-on demos, we understood the differences between model inputs, model parameters, and model hyperparameters, and then discussed hyperparameter tuning support in scikit-learn using the GridSearchCV function. Our hands-on demos included hyperparameter tuning for the lasso regression model where we tuned the hyperparameter alpha, the coefficient of the penalty function; we saw that different regression models have different configuration properties that you can refine and tune to find the best candidate model. With this, we come to the very end of this course on building regression models in scikit-learn. If you're interested in machine learning and you're interested in the scikit-learn library, here are some other courses on Pluralsight that you can watch: Building Classification Models with scikit-learn, and Building Clustering Models with scikit-learn. And that's it from me here today. I hope you had fun watching this course, thank you for listening
